[{"categories":null,"contents":"I started using OPA at my $dayJob recently and there are some parts that I think is not intuitive to grok for beginners.\nFirst, what is the relationship between OPA, Rego, and conftest? Rego is a declarative language used to write OPA policies. Then, OPA is the engine that takes in the policies written in Rego and evaluates it, producing a set of documents called \u0026ldquo;rules\u0026rdquo;. You can use OPA and the Rego language directly to write policies for your config files but using conftest will make the DX much better.\nRego Basics The Rego language focuses on querying the input to look for a given condition. If the input satisfies the query, then it will produce the document.\nVariable Assignments Variable assignment in Rego works the same like in other language. The expression foo := \u0026quot;hello\u0026quot; will assign the value \u0026quot;hello\u0026quot; to the variable foo.\nOne difference in Rego is that it implicitly assigns value true to the document if the condition given evaluates to true. In the example below, there\u0026rsquo;s two ways to write the Rego expression. Rego actually implicitly assigns the value true so we can also remove\nfoo := \u0026#34;hello\u0026#34; # first way: explicitly assigns `true` to `result` when condition is satisfied result := true if foo == \u0026#34;hello\u0026#34; # Rego implicitly assigns `true` to `result` when condition is satisfied result if foo == \u0026#34;hello\u0026#34; Let\u0026rsquo;s bring this up to next level. Most of the time, the condition you\u0026rsquo;re checking is not as straight forward as checking the value against a static value. You might also need to evaluate expressions in between and save the intermediary values in a variable to help improve readability. In previous example we only used a one-liner for the rule body but you can also have more complex rule body like the following using curly braces.\nDeclarative Rego Language The Rego language is declarative and useful to query data structures for any value. Consider the following example (Rego playground link):\nLet\u0026rsquo;s assume our input is an array of object, each containing the keys \u0026ldquo;id\u0026rdquo; and \u0026ldquo;name\u0026rdquo;. In this policy we\u0026rsquo;re checking that the objects doesn\u0026rsquo;t have any forbiden value for \u0026ldquo;name\u0026rdquo;.\nforbidden_names := [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden if input.users[i].name == forbidden_names[j] This code would look something like this in Python:\n1 2 3 4 5 6 7 8 9 users = [{\u0026#34;name\u0026#34;: \u0026#34;foo\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;bar\u0026#34;}] forbidden_names = [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden = [] for i in range(len(input.users): for j in range(len(forbidden_names)): user_forbidden.push(input.users[i].name == forbidden_names[j]) return any(user_forbidden) For both codes, user_forbidden will evaluate to true if one of the user name is included in the forbidden_names list. In the Python code, we used for loops with the any() function to check that none of the value is true. In the Rego code, we don\u0026rsquo;t have to use any for loop or iterate through the user list. forbidden_names[i] means \u0026ldquo;for any of the values in forbidden_names. So in our Rego code, we essentially tells OPA, if any of the value in input.users is the same as any of the value in forbidden_name, then return set the value of user_forbidden to true.\nIn this case, since we are not using the index i and j to reference the value at those index anywhere in the policy, we can simplify it more by using _ (underscore) instead for the index. _ is like a throwaway value and we don\u0026rsquo;t care about the index, we just care if one of the values is the same in user.input and forbidden_names.\nuser_forbidden if input.users[_].name == forbidden_names[_] More complex policies Before this our policies are all simple one liner but Rego also supports writing the rule body in multiple lines. In the example below, we are adding an exception to the rule that the previous rule doesn\u0026rsquo;t apply to user with id == 5. So if one our user name value is john but have id == 5 then user_forbidden won\u0026rsquo;t evaluate to true. Note that we are using the same index i when accessing the name and id property. This means we are referring to the same user. If we use _ or a different index when accessing the name and id, the rule will evaluate to true.\nIf any of the expressions inside the rule body evaluates to false or undefined then it will stop evaluating the rule body and return undefined for user_forbidden.\nforbidden_names := [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden if { input.users[i].name == forbidden_names[_] input.users[i].id != 5 false print(\u0026#34;this will not be printed\u0026#34;) } Using conftest Previously, we used arbitrary names for our rules but conftest introduces a few keywords that we must use so that it can detect any failed rules and includes it in the output. Conftest will pick up any rules with name deny, warn, or violation and the summary will be shown in conftest output.\n➜ tree conftest conftest ├── input.json └── policy └── names.rego # input.json { \u0026#34;users\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;bar\u0026#34; }, { \u0026#34;id\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;foobar\u0026#34; } ] } # policy/names.rego package main import future.keywords.contains import future.keywords.if deny contains msg if { forbidden_names := [\u0026#34;john\u0026#34;] name := input.users[_].name name == forbidden_names[_] msg := sprintf(\u0026#34;username %v is not allowed\u0026#34;, [name]) } warn contains msg if { id := input.users[_].id id == 2 msg := sprintf(\u0026#34;id %v is not allowed\u0026#34;, [id]) } Run conftest against our input file:\n➜ conftest test input.json --policy policy/ WARN - input.json - main - id 2 is not allowed FAIL - input.json - main - username john is not allowed 2 tests, 0 passed, 0 warnings, 2 failures, 0 exceptions Note the values output here, the deny rule will be output as FAIL if the rule passes while the warn rule is counted as WARN. Here, conftest takes the output values from the OPA engine and formats the output for us to make it easier to interpret or integrate with other tools. You can also change the output format of conftest by passing in the --output flag. I like the github output since it will automatically prints the output in a format that Github Actions understoods and will surface error in Github UI approriately. You can also output it as JSON, which is great if you want to process the result output using tools like jq.\n➜ conftest test --help [...] -o, --output string Output format for conftest results - valid options are: [stdout json tap table junit github] (default \u0026#34;stdout\u0026#34;) JSON output:\n➜ conftest test input.json --output json [ { \u0026#34;filename\u0026#34;: \u0026#34;input.json\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;successes\u0026#34;: 0, \u0026#34;warnings\u0026#34;: [ { \u0026#34;msg\u0026#34;: \u0026#34;id 2 is not allowed\u0026#34; } ], \u0026#34;failures\u0026#34;: [ { \u0026#34;msg\u0026#34;: \u0026#34;username john is not allowed\u0026#34; } ] } ] parsers: using other format as input files Until now all our input has been in JSON format but conftest also has built-in parsers that can automatically detect the input format and converts it to JSON for us. As of this moment, here is the list of valid parsers: [cue dockerfile edn hcl1 hcl2 hocon ignore ini json jsonnet properties spdx toml vcl xml yaml dotenv].\nExample is for HCL2 code used for Terraform:\n# input.tf resource \u0026#34;aws_imaginary_resource\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;this\u0026#34; instance_type = \u0026#34;r5.4xlarge\u0026#34; security_groups = [\u0026#34;12345\u0026#34;, \u0026#34;45678\u0026#34;] } resource \u0026#34;aws_imaginary_resource\u0026#34; \u0026#34;that\u0026#34; { name = \u0026#34;that\u0026#34; instance_type = \u0026#34;t3.medium\u0026#34; ingress { port = 1234 cidr = [\u0026#34;0.0.0.0/0\u0026#34;] } } We can use conftest parse to see how conftest will parse the Terraform file and then write our policy based on the parsed input.\n➜ conftest parse input.tf { \u0026#34;resource\u0026#34;: { \u0026#34;aws_imaginary_resource\u0026#34;: { \u0026#34;that\u0026#34;: { \u0026#34;ingress\u0026#34;: { \u0026#34;cidr\u0026#34;: [ \u0026#34;0.0.0.0/0\u0026#34; ], \u0026#34;port\u0026#34;: 1234 }, \u0026#34;instance_type\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;that\u0026#34; }, \u0026#34;this\u0026#34;: { \u0026#34;instance_type\u0026#34;: \u0026#34;r5.4xlarge\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;this\u0026#34;, \u0026#34;security_groups\u0026#34;: [ \u0026#34;12345\u0026#34;, \u0026#34;45678\u0026#34; ] } } } } ","date":"Nov 12","permalink":"https://pokgak.xyz/articles/opa-conftest/","tags":["OPA","conftest","policy"],"title":"Getting started with OPA and conftest"},{"categories":null,"contents":"Note: I\u0026rsquo;m using pseudocode in the code example in this article to keep the article brief. Please refer to the official Slack and OpenTelemetry documentation for the actual code.\nI\u0026rsquo;ve talked about the basics of OpenTelemetry in my previous article. In this one, I\u0026rsquo;ll explain more on how we\u0026rsquo;re integrating OpenTelemetry with our Slack-based application.\nAt the end of this article, this is roughly how the span lifetime and events created will look like:\nSlack BoltJS Socket Mode Compared to the a standard HTTP request/response, we\u0026rsquo;re using BoltJS with socket mode. This gives us the advantage of not having the application exposed publicly to be able to accept requests from Slack but this also means that we cannot just use the auto-instrumentation for HTTP developed by the community.\nSocket mode uses WebSocket to establish connection to Slack and exchange messages through that connection. There is no official auto-instrumentation support for the ws library that is used by BoltJS socket mode but I found opentelemetry-instrumentation-ws, a 3rd-party library for ws library auto-instrumentation.\nSpent a few days integrating it into our application and in the end I concluded that the auto-instrumentation provided by the opentelemetry-instrumentation-ws is too low-level. Our goal is to track user interactions with the application - when they use the bot, which option they choose, what were they trying to do, and whether the interaction ends successfully or with an error. The library, however, created spans when a new connection is established between our application and Slack but no spans or events for user interactions.\nSo, the conclusion? We\u0026rsquo;ll instrument the application manually.\nCreating and Ending Spans Since this application is used company-wide, it\u0026rsquo;s highly likely that multiple users will be using it in parallel. To track user interactions independent from each other, we\u0026rsquo;ll also need separate spans for each user.\nI decided to go with an object spanStore storing the user spans. Like a singleton pattern, a new span will be created for that user if it doesn\u0026rsquo;t exist yet in spanStore, otherwise it will just return the existing user span.\n1 2 3 4 5 6 7 8 9 10 11 12 spanStore = {} function getUserSpan(username) { if (user in spanStore) { return spanStore[username] } span = startSpan(username) spanStore[username] = span return span } Now that we have a function to create the span, when in the lifetime of the incoming event do we create the span? Ideally, as early as possible before anything else so that we can track everything. BoltJS supports setting a global middleware that will be called before the event handler function are called. This is where I call the getUserSpan() function above. For the first event for that user, it will create a new span and for the next events it will just return the existing spans that I can use.\nNext, when do you end the span? Due to how the application works, we\u0026rsquo;re assuming that each user can only have one session at one time and at the end there will always be an finishing event triggered when the user finished their interaction with the application. Based on that fact, I wrote an event listener that will respond to this finishing event by calling the OTel function to end the span and remove the span from the spanStore object above.\n1 2 3 4 5 app.event({id: \u0026#39;finishing_event\u0026#39;}, async ({username}) =\u0026gt; { span = getUserSpan(username) span.end() removeSpanFromSpanStore(span) }) Tracking User Actions with Span Events With these, we have a separate span for each user for the whole duration of their interaction with the application. With only one span, we don\u0026rsquo;t have insights yet into what the user are doing, which actions are taken by the user, so we\u0026rsquo;ll need a way to track user actions.\nWith Slack BoltJS, we can trigger a listener function on every user interaction. I wrote a function that will create a new span event using the user input id as the event name. I also passed in the whole payload so that the we can see the payload of user actions later when debugging issues. Add this as another global middleware, now we\u0026rsquo;re creating a new event for every user actions.\n1 2 3 4 app.action(\u0026#39;callback_id\u0026#39;, async ({username, action_id, payload}) =\u0026gt; { span = getUserSpan(username) span.addEvent(action_id, {payload}) }) Confession I\u0026rsquo;m actually not convinced that my way of doing this is correct. One of the reason is that since I use one root span for the whole interaction for a user, I\u0026rsquo;m also tracking the duration taken by the user to do the next action. From our perpective, this made the duration of the span tracked is now kinda useless for us since it also includes factors that are not controllable by us (time taken for users to do the next action).\nInstead of one root span and creating new span events for every user interaction, maybe a new span for each interaction, linked to the previous span would be better since we only track the duration that we are in control of, not how long the user takes to click a button.\nNevertheless, since I already implemented like this now, let\u0026rsquo;s see how that will turn out. Like the saying, you either die a hero, or you live long enough to see yourself become the villain.\n","date":"Aug 13","permalink":"https://pokgak.xyz/articles/otel-slack-integration/","tags":["Observability","otel-slack"],"title":"Instrumenting a Slack bot with OpenTelemetry"},{"categories":null,"contents":"I got to work on integrating OpenTelemetry in an application that our team maintains recently so I\u0026rsquo;m starting a series documenting my learnings throughout this journey.\nA little background info on the application I\u0026rsquo;m working on: it\u0026rsquo;s a Slack chatbot written in Typescript using BoltJS. Our goal is to know how many users are using our Slack bot with a breakdown of the percentage of successful and error interactions. When an error happened, we also want to know what exactly the user did and the current state of the application that caused it to error. Based on my reading, the last sentence is exactly what observability promises, So that\u0026rsquo;s why we\u0026rsquo;re giving it a try.\nOpenTelemetry can be divided into three categories: tracing, logging, and metrics; but I\u0026rsquo;ll be focusing on tracing in this series.\nTracing Primers To get started you should know some basic concepts about tracing.\nTraces, Spans A trace consists of multiple spans and a span is a unit of work with a start and end time. In a span, you can create events that marks when something happened in the lifetime of the span.\nA span can also have nested spans and these are called child spans. The parent span is usually representing some abstract unit of work, like the lifetime of a HTTP request when it from when it hits the application until the response is sent. Child spans can be used to get more details into the operations done during the lifetime of that parent span ie. API call to another service to fetch more informations.\nSpan attributes, Status, Errors To add context to the spans, you can set custom attributes. Ideally, you want to send all the information that will help when debugging your application in the future so that later you don\u0026rsquo;t have to modify the code and add more attribute when you noticed an issue and realized that you don\u0026rsquo;t have enough information to debug the issue.\nIf your application encounters an error, you can set the span status to ERROR and also add the stack trace to the context for use in debugging. By default your span status will be set to OK.\nSpan Exporter After the span ends, you\u0026rsquo;ll want to send it to a backend service that will store and process it so that you can use it later. The sending is done by OTel Exporters. There are multiple backend available that accepts OTel traces as inputs but such as Jaeger, Zipkin but for my testing I\u0026rsquo;m using Honeycomb with the OLTP Collector.\nDebugging For debugging, there\u0026rsquo;s also the ConsoleSpanExporter which will print out your spans in the console instead of sending it anywhere. I find this very useful to get fast response on what is being sent over but it\u0026rsquo;s hard to do analysis with it so in production environment you should configure the exporter to use other backends instead.\nAutomatic vs Manual Instrumentation Now we got the basics out of the way, let\u0026rsquo;s look at how you can start adding spans to your application to build traces.\nThe easiest way to get started is to use auto instrumentation which will automatically injects code in the HTTP, requests, DNS, libraries that you\u0026rsquo;re using to create spans and events. In nodejs, this can be done by installing the auto-instrumentations-node NPM package. This package pulls in several other packages to automatically instrument your application.\nThis is a nice onboarding experience but I get overwhelmed by the amount of data sent when by these auto instrumentation package. Therefore, I recommend to you to start with manual instrumentation instead.\nWith manual instrumentation, you\u0026rsquo;re forced to be intentional with the data that you\u0026rsquo;re sending to the backend. With this I get to decide which information I want to send over and already have in mind what I want to do with it and which information I would like to gain from it.\nInitialization Whatever approach you end up with for the instrumentation, you\u0026rsquo;ll want to make sure that you\u0026rsquo;re initializing the OTel libraries at the start of your application. This is required because if you starts it later, your application might already be handling request when your OTel libraries are not initialized yet, causing it to miss some requests, or worse encounter errors.\nThe recommended way to do it is to use the -r flag from the node command:\n-r, \u0026ndash;require module Preload the specified module at startup. Follows require()\u0026rsquo;s module resolution rules. module may be either a path to a file, or a Node.js module name.\nSo in your package.json you\u0026rsquo;ll have to add that to your start command:\n1 2 3 scripts: { \u0026#34;start\u0026#34;: \u0026#34;node -r ./tracing.js app.js\u0026#34;, } If you\u0026rsquo;re using Typescript like me, you\u0026rsquo;ll want to use the NODE_OPTIONS shell variable to specify the flag instead:\n1 2 3 scripts: { \u0026#34;start\u0026#34;: \u0026#34;NODE_OPTIONS=\u0026#39;-r ./tracing.js\u0026#39; ts-node app.ts\u0026#34;, } NodeSDK vs NodeTracerProvider Confusion One thing that made me confused is how different the code for initializing auto instrumentation compared to manual instrumentation.\nThis is the code provided by Honeycomb to use auto instrumentation. The key there is the getNodeAutoInstrumentation() function which will register all the supported auto instrumentation libraries. One more thing is that it is using the NodeSDK class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // tracing.js (\u0026#34;use strict\u0026#34;); const { NodeSDK } = require(\u0026#34;@opentelemetry/sdk-node\u0026#34;); const { getNodeAutoInstrumentations } = require(\u0026#34;@opentelemetry/auto-instrumentations-node\u0026#34;); const { OTLPTraceExporter } = require(\u0026#34;@opentelemetry/exporter-trace-otlp-proto\u0026#34;); // The Trace Exporter exports the data to Honeycomb and uses // the environment variables for endpoint, service name, and API Key. const traceExporter = new OTLPTraceExporter(); const sdk = new NodeSDK({ traceExporter, instrumentations: [getNodeAutoInstrumentations()] }); sdk.start() On the other hand, this is the code example from opentelemetry.io to start manual instrumentation. Notice that it\u0026rsquo;s not using the NodeSDK class anymore and you need to create the Resource and NodeTracerProvider objects and configure it yourself.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 const opentelemetry = require(\u0026#34;@opentelemetry/api\u0026#34;); const { Resource } = require(\u0026#34;@opentelemetry/resources\u0026#34;); const { SemanticResourceAttributes } = require(\u0026#34;@opentelemetry/semantic-conventions\u0026#34;); const { NodeTracerProvider } = require(\u0026#34;@opentelemetry/sdk-trace-node\u0026#34;); const { registerInstrumentations } = require(\u0026#34;@opentelemetry/instrumentation\u0026#34;); const { ConsoleSpanExporter, BatchSpanProcessor } = require(\u0026#34;@opentelemetry/sdk-trace-base\u0026#34;); // Optionally register automatic instrumentation libraries registerInstrumentations({ instrumentations: [], }); const resource = Resource.default().merge( new Resource({ [SemanticResourceAttributes.SERVICE_NAME]: \u0026#34;service-name-here\u0026#34;, [SemanticResourceAttributes.SERVICE_VERSION]: \u0026#34;0.1.0\u0026#34;, }) ); const provider = new NodeTracerProvider({ resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor); provider.register(); TBH I\u0026rsquo;m still not clear what is the difference betwen using NodeSDK vs manually configuring the NodeTracerProvider. When using NodeSDK does the NodeTracerProvider got configured automatically?\nHow and when to start tracing? To start manually instrumenting your application, you\u0026rsquo;ll have to create a root span. A root span is the first span you create once the request enters your application.\nNow, if you have a normal HTTP request/response-based application, it is easy to figure out where to start and end your root spans. All your incoming requests will most likely be handled by a controller and each endpoint will be handled by a method. In this type of application, your root span can be started once the request hits the application in the method in your controller and ends before you send the response.\nDuring the lifetime of that request, you can create child spans to track other works done while processing the request. There\u0026rsquo;s only one entry point for requests and exiting the entry point means the request is finished. If your application encountered errors during the execution, it can set the span status to ERROR and add the stack trace info to the span.\nConclusion Once you managed to create spans, set attributes, and then export it to a backend. You\u0026rsquo;re pretty much done with the basics of instrumenting your application. Go ahead and add more traces to your application!\n","date":"Aug 13","permalink":"https://pokgak.xyz/articles/otel-basics/","tags":["observability","opentelemetry","otel-slack","honeycomb"],"title":"OpenTelemetry Basics"},{"categories":null,"contents":"HCL adalah bahasa yang digunakan dalam produk-produk daripada Hashicorp seperti Terraform dan Packer. Kebiasaannya, fail HCL ini ditulis secara manual tetapi jika anda ingin menulis atau mengubah fail-fail tersebut secara programmatik menggunakan code, maka anda boleh menggunakan hclwrite, sebuah library yang ditulis dalam Go.\nBlog post ini dibahagikan kepada dua bahagian. Bahagian pertama menunjukkan cara untuk menghasilkan block baru from scratch dan simpan ke fail. Ini adalah asas untuk bahagian kedua di mana kita akan mengubah fail HCL sedia ada dan memastikan format fail tersebut terjaga dan tidak melakukan pengubahan secara semberono.\nSaya tidak akan memberi penerangan penuh syntax fail HCL kerana ia boleh didapati di halaman ini.\nBahagian 1: Cipta block baru daripada mula Untuk bahagian 1 ini, kita akan belajar cara untuk:\ncipta block baru tambah attribute dalam block tersebut simpan block yang dicipta ke dalam fail Untuk contoh pertama, kita akan cuba generate block HCL di bawah:\n1 2 3 4 resource \u0026#34;github_membership\u0026#34; \u0026#34;user\u0026#34; { username = \u0026#34;github_username\u0026#34; role = \u0026#34;member\u0026#34; } Inilah code yang diperlukan untuk generate block tersebut:\n1 2 3 4 5 6 7 8 9 newMemberBlock := hclwrite.NewBlock(\u0026#34;resource\u0026#34;, []string{\u0026#34;github_membership\u0026#34;, mlId}) body := newMemberBlock.Body() body.SetAttributeValue(\u0026#34;username\u0026#34;, cty.StringVal(githubUsername)) body.SetAttributeValue(\u0026#34;role\u0026#34;, cty.StringVal(\u0026#34;members\u0026#34;)) f := hclwrite.NewEmptyFile() f.Body().AppendBlock(newMemberBlock) f.Body().AppendNewline() ioutil.WriteFile(\u0026#34;data/result_members.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Cipta block Mula-mula kita perlukan sebuah block untuk mengisi content-content lain ke dalamnya. Ini boleh dicipta menggunakan function hclwrite.NewBlock(). Parameter pertama function ini adalah nama type, kemudian diikuti dengan label-label bagi block tersebut. Dalam contoh block di atas, nama type yang kita perlukan adalah \u0026ldquo;resource\u0026rdquo; dan kita memerlukan label \u0026ldquo;github_membership\u0026rdquo; dan \u0026ldquo;user\u0026rdquo;.\nSeterusnya kita boleh mula mengisi boleh yang baru sahaja kita cipta tadi. Dalam contoh di atas, block itu mengandungi attribute \u0026ldquo;username\u0026rdquo; dan \u0026ldquo;role\u0026rdquo; dengan nilai masing-masing. Kita boleh set attribute sesebuah block dengan function SetAttributeValue().\nUntuk nama attribute, kita boleh menggunakan string biasa tetapi bagi nilai attribute tersebut, hclwrite menggunakan library cty (sebut: si-tai) untuk memastikan nilai attribute tersebut mempunyai type yang betul setelah habis proces pemprosesan nanti. Bagi memasukkan nilai string menggunakan library cty, kita boleh menggunakan function cty.StringVal(), yang akan menukarkan string Go biasa kepada nilai cty yang setaraf.\nSimpan block ke dalam fail 1 2 3 4 f := hclwrite.NewEmptyFile() f.Body().AppendBlock(newMemberBlock) f.Body().AppendNewline() ioutil.WriteFile(\u0026#34;data/result_members.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Dengan itu selesai bahagian pertama iaitu mencipta block tersebut menggunakan code. Seterusnya, kita perlu menyimpan block yang telah kita cipta ini ke dalam fail. Untuk memudahkan, kali ini kita akan bermula dengan fail baru yang kosong. Untuk bermula dengan fail kosong, kita boleh menggunakan function hclwrite.NewEmptyFile(). Fuction ini seolah-olah memberi kita kanvas kosong untuk kita isikan dengan block-block yang akan kita reka.\nUntuk menambah block ke fail tersebut, kita tidak boleh menambahnya terus ke objek File yang dipulangkan oleh function NewEmptyFile. Semua content dalam sebuah fail perlu diletakkan dalam bahagian Body block tersebut. Kita boleh mengakses Body melalui function Body().\nSeterusnya, kita boleh tambah block yang telah kita siapkan dalam bahagian sebelum ini menggunakan function ApppendBlock ke dalam Body yang telah dapat dalam langkah sebelum ini. Untuk memastikan block kita itu nampak kemas, maka kita boleh tambah baris kosong di hujung fail dengan menggunakan function AppendNewLine.\nAkhirnya, untuk menyimpan semua yang telah kita generate ini ke fail, kita boleh menggunakan function ioutil.WriteFile(). Kita boleh memasukkan content fail kita dengan cara menukarkannya kepada bytes. hclwrite juga mempunyai function Format untuk memastikan fail yang telah kita cipta itu mematuhi recommended format untuk sesebuah fail HCL. Selepas itu anda bolehlah menyemak fail HCL yang dihasilkan di lokasi yang telah diberi semasa memanggil function WriteFile tadi.\nBahagian 2: Mengubah block sedia ada Untuk bahagian 2 ini, kita akan belajar cara untuk:\nbaca dan parse fail HCL sedia ada cari bahagian untuk kita ubah tambah pengubahan yang diinginkan menggunakan Token beza Traversal dan Value Fail yang ingin kita hasilkan adalah seperti berikut:\n1 2 3 4 5 6 7 8 9 10 module \u0026#34;team_itsm_team\u0026#34; { source = \u0026#34;../../modules/github/team_nx\u0026#34; team_name = \u0026#34;ITSM Team\u0026#34; members = [ github_membership.kasan.username, github_membership.mismail.username, // *kita ingin menambah baris ini ] } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 content, _ := ioutil.ReadFile(\u0026#34;data/\u0026#34; + pod + \u0026#34;.tf\u0026#34;) f, _ := hclwrite.ParseConfig(content, \u0026#34;\u0026#34;, hcl.InitialPos) block := f.Body().FirstMatchingBlock(\u0026#34;module\u0026#34;, []string{\u0026#34;team_\u0026#34; + pod + \u0026#34;_team\u0026#34;}) oldMembers := block.Body().GetAttribute(\u0026#34;members\u0026#34;).Expr().BuildTokens(nil) newEntry := hclwrite.NewExpressionAbsTraversal( hcl.Traversal{ hcl.TraverseRoot{Name: \u0026#34;github_membership\u0026#34;}, hcl.TraverseAttr{Name: mlId}, hcl.TraverseAttr{Name: \u0026#34;username\u0026#34;}, }, ).BuildTokens(nil) newMembers := append( oldMembers[:len(oldMembers)-2], \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, ) newMembers = append(newMembers, newEntry...) newMembers = append(newMembers, hclwrite.Tokens{ \u0026amp;hclwrite.Token{Type: hclsyntax.TokenComma, Bytes: []byte{\u0026#39;,\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte{\u0026#39;]\u0026#39;}}, }...) block.Body().SetAttributeRaw(\u0026#34;members\u0026#34;, newMembers) ioutil.WriteFile(\u0026#34;data/result_itsm.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Baca dan parse fail HCL sedia ada Kali ini kita tidak akan bermula dengan fail kosong, sebaliknya mengambil fail HCL yang sedia ada.\n1 2 content, _ := ioutil.ReadFile(\u0026#34;data/\u0026#34; + pod + \u0026#34;.tf\u0026#34;) f, _ := hclwrite.ParseConfig(content, \u0026#34;\u0026#34;, hcl.InitialPos) Kita menggunakan function ReadFile untuk membaca keseluruhan fail tersebut. Function tersebut akan memulangkan content dalam bentuk []byte yang akan kita berikan kepada function hclwrite.ParseConfig(). Function inilah yang bertanggungjawab memahami syntax sedia ada fail HCL tersebut dan membolehkan kita mengubah fail itu dengan tepat. Function ini akan memulangkan objeck hclwrite.File, sama seperti function hclwrite.NewEmptyFile() di bahagian 1.\nCari bahagian untuk kita ubah Terdapat pelbagai cara yang boleh kita gunakan untuk mencari bahagian tertentu yang ingin kita ubah. Antaranya ialah dengan menggunakan function FirstMatchingBlock(). Kita perlu menetapkan jenis (type) block yang ingin dicari, kemudian diikuti dengan label-label yang ada pada block tersebut.\n1 block := f.Body().FirstMatchingBlock(\u0026#34;module\u0026#34;, []string{\u0026#34;team_\u0026#34; + pod + \u0026#34;_team\u0026#34;}) Tambah pengubahan yang diinginkan 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 oldMembers := block.Body().GetAttribute(\u0026#34;members\u0026#34;).Expr().BuildTokens(nil) newEntry := hclwrite.NewExpressionAbsTraversal( hcl.Traversal{ hcl.TraverseRoot{Name: \u0026#34;github_membership\u0026#34;}, hcl.TraverseAttr{Name: mlId}, hcl.TraverseAttr{Name: \u0026#34;username\u0026#34;}, }, ).BuildTokens(nil) newMembers := append( oldMembers[:len(oldMembers)-2], \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, ) newMembers = append(newMembers, newEntry...) newMembers = append(newMembers, hclwrite.Tokens{ \u0026amp;hclwrite.Token{Type: hclsyntax.TokenComma, Bytes: []byte{\u0026#39;,\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte{\u0026#39;]\u0026#39;}}, }...) block.Body().SetAttributeRaw(\u0026#34;members\u0026#34;, newMembers) Dapatkan nilai attribute yang ingin kita ubah melalui function GetAttribute(). Nilai attribute ini merupakan sebuah expression. Untuk mengubahnya kita perlu menukarkannya kepada Token.\nApa itu Token? TODO\nBeza Traversal dan literal Value Traversal digunakan untuk merujuk kepada variable lain dalam fail HCL tersebut. Literal value tidak merujuk kepada mana-mana bahagian lain dalam fail/projek, berdiri dengan sendiri.\nSimpan fail yang diubah 1 ioutil.WriteFile(\u0026#34;data/result_itsm.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Konklusi Manipulasi fail HCL menggunakan library hclwrite lebih kompleks daripada melakukan ubahsuai secara manual tetapi jika ini perkara yang anda perlu lakukan setiap hari, mungkin lebih senang jika anda meluangkan masa beberapa hari untuk membangunkan solusi automation ini supaya perkara yang sama tidak perlu lagi intervensi manual daripada anda.\nSumber Rujukan Terraform Configuration Syntax hclwrite package documentation Write Terraform Files in Go with hclwrite ","date":"Sep 19","permalink":"https://pokgak.xyz/articles/hclwrite-basics/","tags":null,"title":"Generate fail HCL menggunakan library hclwrite"},{"categories":null,"contents":"Dalam blog post saya sebelum ini saya dah menerangkan bagaimana saya membuat graf animasi perkembangan status pemberian imunisasi negeri-negeri di Malaysia. Seterusnya saya juga ada berkongsi asas-asas untuk menggunakan Github Actions. Dalam blog post ini saya ingin menerangkan pula bagaimana saya menggunakan Github Actions untuk mengemaskini graf tersebut dengan data terbaru yang dikeluarkan oleh pihak CITF Malaysia secara automatik setiap hari.\nKonfigurasi penuh Sebelum saya mula penerangan, inilah hasil fail workflow Github Actions yang saya gunakan:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 name: Update Graphs on: push: branches: [main] workflow_dispatch: schedule: - cron: \u0026#34;0 0 * * *\u0026#34; # Run workflow everyday at 12 AM jobs: vax-count-by-state: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: token: ${{ secrets.GITHUB_TOKEN }} - uses: actions/setup-python@v2 with: python-version: \u0026#34;3.8\u0026#34; - name: Cache pip uses: actions/cache@v2 with: # This path is specific to Ubuntu path: ~/.cache/pip # Look to see if there is a cache hit for the corresponding requirements file key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;requirements.txt\u0026#39;) }} restore-keys: | ${{ runner.os }}-pip- ${{ runner.os }}- - name: Install dependencies run: pip3 install -r requirements.txt - name: Fetch latest data \u0026amp; generate new graph run: python3 main.py - id: get-date run: echo \u0026#34;::set-output name=value::$(date --iso-8601)\u0026#34; - uses: stefanzweifel/git-auto-commit-action@v4 with: commit_message: \u0026#34;bot: update graph for ${{ steps.get-date.outputs.value }}\u0026#34; Bahagian-bahagian Saya akan memecahkan penerangan saya kepada beberapa bahagian iaitu:\nJadual auto-update Melakukan kemaskini graf Commit \u0026amp; Push kemaskini ke repository Bahagian 1: Jadual Auto-update Di bahagian ini saya akan menunjukkan cara bagaimana saya menetapkan Github Actions untuk melakukan kemaskini setiap hari secara automatik.\nDalam penerangan saya berkenaan asas-asas Github Actions, saya ada menyebut yang sesebuah workflow itu boleh dicetuskan oleh pelbagai event daripada Github. Antara event yang disokong adalah menjalankan workflow tersebut berdasarkan jadual yang ditetapkan. Untuk ini kita memerlukan keyword schedule di bawah keyword utama on seperti contoh di bawah:\n1 2 3 on: schedule: - cron: \u0026#34;0 0 * * *\u0026#34; # Run workflow everyday at 12 AM Keyword schedule ini menerima jadual dalam format syntax cron. Jika anda tahu selok-belok sesebuah sistem UNIX atau Linux anda mungkin tahu mengenai cron. Untuk yang belum tahu apa syntax cron itu, ia mempunyai 5 bahagian yang dipisahkan dengan paling kurang satu karakter whitespace seperti space atau tab. Bermula dari kiri, bahagian-bahagian tersebut melambangkan nilai berikut, nilai yang boleh diterima saya letakkan dalam kotak disebelah:\nminit [0 hingga 59] jam [0 hingga 23] hari dalam bulan [1 hingga 31] bulan dalam tahun [1 hingga 12] hari dalam minggu [0 hingga 6], bermula dengan 0=Ahad, 1=Isnin, dan seterusnya hingga 6=Sabtu Nilai khas * boleh digunakan yang membawa maksud untuk setiap nilai dalam bahagian tersebut. Dalam fail workflow saya jadual cron yang digunakan adalah \u0026ldquo;0 0 * * *\u0026rdquo; yang bermakna, \u0026ldquo;Jalankan fail workflow ini pada jam 0:00 (tengah malam) setiap hari dalam bulan, untuk setiap tahun, tidak mengira hari apa pun\u0026rdquo;. Kadangkala syntax cron ini boleh mengelirukan. Jadi saya mencadangkan laman crontab.guru untuk memeriksa dan bereksperimen dengan syntax cron ini.\nBahagian 2: Melakukan kemaskini graf Di blog post sebelum ini saya telah menerangkan code yang saya gunakan untuk menjana graf animasi baru jadi kita akan menggunakan skrip yang sama untuk melakukannya di sini. Walaupun begitu, sebelum menjalankan skrip Python untuk menjana graf berdasarkan informasi baru, kita perlu menyediakan semua perisian yang diperlukan oleh skrip tersebut di Github Actions Runner.\nUntuk itu saya menggunakan [actions/setup-python] untuk menyediakan Python di runner tersebut dan seterusnya menginstall dependency lain. Hanya step terakhir dalam job tersebut adalah bahagian dimana saya betul-betul menjalan kerja tersebut. Berikut adalah code tersebut.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - uses: actions/checkout@v2 with: token: ${{ secrets.GITHUB_TOKEN }} - uses: actions/setup-python@v2 with: python-version: \u0026#34;3.8\u0026#34; - name: Cache pip uses: actions/cache@v2 with: # This path is specific to Ubuntu path: ~/.cache/pip # Look to see if there is a cache hit for the corresponding requirements file key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;requirements.txt\u0026#39;) }} restore-keys: | ${{ runner.os }}-pip- ${{ runner.os }}- - name: Install dependencies run: pip3 install -r requirements.txt - name: Fetch latest data \u0026amp; generate new graph run: python3 main.py Bahagian 3: Commit dan Push kemaskini ke repository Setelah mencipta graf baru dari data terkini daripada repo CITF-public, graf baru kita sudah pun ready tapi belum lagi dipaparkan di website https://pokgak.github.io/citf-graphs kerana ia masih belum dicommit lagi ke repository.\nBiasanya saya akan melakukan commit secara manual dan push ke Github tapi oleh kerana kita melakukan semua proses diatas secara automatik daripada Github Actions, kita tidak boleh lagi buat begitu. Oleh itu, saya menggunakan Actions stefanzweifel/git-auto-commit-action untuk melakukan commit secara automatik. Berikut adalah segmen fail workflow saya yang menunjukkan penggunaan Actions ini:\n1 2 3 - uses: stefanzweifel/git-auto-commit-action@v4 with: commit_message: \u0026#34;bot: update graph for ${{ steps.get-date.outputs.value }}\u0026#34; Seperti yang anda boleh lihat, mudah sahaja cara penggunaan actions ini. Kita hanya perlu menggunakan keyword uses untuk menanda bahawa kita ingin menggunakan Actions luar dalam fail workflow ini, diikuti dengan nama Actions tersebut. Senarai semua actions yang ada boleh dilihat di Github Actions Marketplace. Tambahan pula, anda juga boleh menulis Actions anda sendiri!.\nKonklusi Sejak adanya workflow ini, saya tidak perlu lagi memastikan graf yang saya hasilkan di pokgak/citf-graphs sentiasa dikemaskini dengan maklumat terbaru secara manual, semuanya dilakukan secara automatik. Sejak itu juga, Github menunjukkan aktiviti saya aktif setiap hari, walaupun pada hakikatnya itu semua adalah bot sahaja :p\n","date":"Sep 02","permalink":"https://pokgak.xyz/articles/auto-update-citf-graphs/","tags":["Github Actions","CI/CD"],"title":"Auto-update Graf Covid-19 menggunakan Github Actions"},{"categories":null,"contents":"Perkataan \u0026ldquo;cloud\u0026rdquo; rasanya tak asing lagi dalam kamus anak muda zaman sekarang tapi kebanyakannya merujuk kepada \u0026ldquo;cloud storage\u0026rdquo; iaitu servis penyimpanan data online. Kali ini saya ingin menerangkan konsep cloud computing dari sudut seorang programmer.\nApa itu cloud? \u0026lsquo;The cloud is just someone else\u0026rsquo;s computer\u0026rsquo;, adalah salah satu meme yang banyak ditemui online. Hakikatnya begitulah, gambar yang diupload ke iCloud, website Twitter yang anda akses setiap hari, video yang anda tonton di YouTube, semuanya dilayan (hosted) oleh komputer-komputer yang tersusun di data center besar di seluruh dunia.\nOkay, tapi cloud itu bukan sekadar timbunan komputer seluas berpuluh-puluh padang bola sahaja. Ada beberapa ciri-ciri penting untuk sesuatu itu dipanggil cloud computing.\nCiri-Ciri Cloud Computing Berikut adalah ciri-ciri cloud computing yang utama bagi saya:\nsumber atas permintaan (on-demand resources) kitaran maklum balas pendek (short feedback cycle) berskala infinity (infinitely scalable) ketersediaan global (global availability) Sumber Atas Permintaan (on-demand resources) Ciri ini adalah kelebihan utama apabila menggunakan servis cloud computing. Sebarang sumber yang diperlukan untuk menyebarkan aplikasi anda sama ada server, sistem penyimpanan atau pengkalan data, kebiasaannya boleh disiapkan dalam masa kurang daripada 30 minit. Dengan ini, apabila aplikasi itu telah siap dibangunkan, ia boleh siap tersedia untuk disebarkan dalam masa yang amat singkat.\nDalam artikel Alkisah Syarikat A saya ingin menggambarkan bagaimanakah proses ini dilakukan sebelum ini. Terdapat pelbagai tugas yang perlu dilakukan sebelum sesuatu sumber itu sedia untuk digunakan. Cloud computing mengambil alih tugas ini dari syarikat A.\nKitaran Maklum Balas Pendek (short feedback cycle) Salah satu kelebihan kebolehan menyiapkan sumber atas permintaan adalah kebolehan untuk bertindak dan menyelesaikan masalah kekurangan kapasiti sumber dengan segera. Sebelum ini, masa untuk menyiapkan sumber baru amat besar, oleh sebab itu sumber untuk aplikasi sentiasa disiapkan dengan konfigurasi lebih besar daripada keperluannya hanya supaya mereka mempunyai kapasiti untuk berkembang sebelum perlu dipesan sumber baru.\nTapi dengan kelebihan waktu singkat untuk menyediakan sumber baru, tidak perlu lagi server itu disiapkan dengan konfigurasi lebih dari keperluan. Jika aplikasi itu tiba-tiba mendapat trafik yang tinggi, penyiapan sumber baru untuk menampung kapasiti boleh dilakukan segera.\nBerskala Infiniti (infinitely scalable) Platform-platform hyperscaler cloud computing dikatakan mempunyai sumber infiniti. Pada hakikatnya, apa-apa sumber tidak boleh dikatakan infiniti kerana sumber asli dunia pasti akan habis suatu hari nanti. Tapi infiniti di sini bermaksud, platform-platform hyperscaler ini mampu berkembang lebih cepat daripada kadar keperluan sumber oleh semua pengguna platform tersebut.\nKetersediaan Global (global availability) Secara teori satu sumber yang disebarkan dari sebuah data center di Malaysia punyai kebolehan untuk melayan permintaan daripada seluruh dunia. Hakikatnya, ini akan meninggalkan kesan kepada pengguna-pengguna yang berada di lokasi bertentangan dengan lokasi di mana aplikasi itu dilayan.\nOleh itu, untuk mengembangkan aplikasi ke taraf global, syarikat-syarikat perlu menyediakan server di seluruh dunia supaya permintaan dari pengguna boleh dilayan daripada data center yang terdekat dengan mereka. Bahkan teknologi seperti edge computing wujud hanya untuk mengurangkan masa untuk melayan permintaan pengguna dengan meletakkan server pelayan sedekat mungkin dengan pengguna.\nTapi untuk sesebuah syarikat itu menyediakan server di sebuah lokasi baru tidaklah mudah, terutamanya jika mereka tiada kehadiran fizikal di negara tersebut. Hal-hal regulasi, pembayaran bil dan sebagainya akan menjadi lebih rumit kerana terdapat transaksi rentas negara.\nPlatform hyperscaler cloud computing seperti AWS, Azure, Oracle dll. memudahkan proses ini. Semua aspek fizikal akan diuruskan pihak mereka. Sebagai syarikat pengguna kita hanya perlu membayar sumber-sumber tersebut. Tidak perlu lagi difikirkan aspek-aspek lain.\nKonklusi 4 ciri-ciri di atas pada hemat saya adalah karakteristik utama sesebuah platform cloud computing. Dengan memahami ciri-ciri ini kita dapat menjadikan cloud computing ini sebagai alat untuk menyelesaikan permasalahan yang wujud dalam menyediakan aplikasi kita.\nPemahaman ini amatlah penting kerana tanpa memahami bagaimana teknologi cloud computing ini mampu membantu menyelesaikan masalah yang kita hadapi, kita hanyalah seperti lembu diikat hidung, ikut sahaja apa trend yang orang lain suapkan. Akhirnya masa dan wang dibazirkan tanpa kita memperoleh manfaat apa-apa pun.\n","date":"Aug 25","permalink":"https://pokgak.xyz/articles/cloud-computing/","tags":["Cloud Computing"],"title":"Apa itu Cloud Computing?"},{"categories":null,"contents":"Saya gemar melayari subreddit r/dataisbeautiful dan melihat graf hasil buatan pengguna Reddit lain di sana. Salah satu jenis graf yang saya paling minat adalah apabila graf itu seolah-olah animasi, berubah selaras mengikut jangka masa waktu yang semakin bertambah. Kita boleh melihat perkembangan sesuatu data itu dari mula hingga ke akhir.\nContoh post terbaru di subreddit itu yang mempunyai graf sebegini adalah seperti graf di bawah yang memaparkan Kadar vaksinasi sebahagian daripada negara-negara di seluruh dunia (sayang Malaysia tidak dimasukkan sekali di sini):\nSebelum ini saya menganggap animasi sebegini rumit untuk dilakukan tetapi apabila pihak CITF telah melancarkan public repo di Github bagi data vaksinasi Malaysia, saya memutuskan untuk cuba menghasilkan semula gaya visualisasi ini menggunakan data tersebut.\nSeterusnya saya akan menerangkan langkah-langkah yang diperlukan untuk menghasilkan visualisasi seperti yang di bawah. Sebagai rujukan, code penuh yang saya gunakan di sini boleh didapati di sini.\nPembersihan Data Dalam projek yang melibatkan data sebegini, data boleh datang dari pelbagai sumber dan bentuk. Oleh itu, langkah pertama selalunya adalah pembersihan data. Tujuan langkah ini adalah supaya pada akhirnya kita mempunyai data dalam format yang sesuai dan boleh terus digunakan untuk langkah seterusnya tanpa perlu pemprosesan ekstra apa-apa pun.\nSaya bernasib baik kali ini kerana sumber data yang dibekalkan oleh pihak CITF Malaysia sudah pun berada dalam format CSV yang senang untuk dibaca menggunakan pandas, sebuah library untuk memanipulasi data menggunakan Python. Pihak CITF tidak menawarkan public REST API yang boleh digunakan untuk mengambil (fetch) data tersebut maka saya terpaksa mengambil data menerusi Github. Proses ini kurang sesuai jika anda mahu menapis dahulu data yang diambil tapi untuk kegunaan saya ini, kaedah ini adalah mencukupi.\n1 2 3 STATE_DATA_URL = \u0026#34;https://raw.githubusercontent.com/CITF-Malaysia/citf-public/main/vaccination/vax_state.csv\u0026#34; df = pd.read_csv(StringIO(requests.get(data_url).text)) Function read_csv akan mengambil output data yang diambil dari Github dan menukarkannya ke format DataFrame yang digunakan oleh library pandas. Format DataFrame adalah 2D seakan-akan Excel. Ia mempunyai rows dan columns yang mempunyai data dan menawarkan fungsi-fungsi untuk memanipulasi data tersebut (gabung, pisah, transpose, etc) dengan mudah. Berikut adalah code yang saya gunakan untuk menyiapkan data raw tadi untuk visualisasi:\n1 2 3 4 5 6 df.set_index([\u0026#34;date\u0026#34;, \u0026#34;state\u0026#34;]) .loc[:, [\u0026#34;cumul_partial\u0026#34;, \u0026#34;cumul_full\u0026#34;, \u0026#34;cumul\u0026#34;]] .rename(columns={\u0026#34;cumul_partial\u0026#34;: \u0026#34;partially_vaxed\u0026#34;, \u0026#34;cumul_full\u0026#34;: \u0026#34;fully_vaxed\u0026#34;}) .sort_values(by=\u0026#34;cumul\u0026#34;, ascending=False) .sort_index(level=\u0026#34;date\u0026#34;, sort_remaining=False) .reset_index() Secara ringkasnya,\nset_index: saya menetapkan column \u0026ldquo;date\u0026rdquo; dan \u0026ldquo;state\u0026rdquo; index DataFrame tersebut yang akan saya gunakan nanti untuk mengasingkan data vaksinasi mengikut tarikh dan negeri loc: pilih hanya column yang saya mahu rename: memberikan nama baharu kepada column-column tersebut supaya lebih mudah difahami sort_values: susun semua data vaksinasi mengikut jumlah kumulatif (\u0026ldquo;cumul\u0026rdquo;) sort_index: susun semua data vaksinasi mengikut tarikh reset_index: menjadikan column index dari langkah 1 sebelum ini balik seperti column biasa yang boleh digunakan secara normal Untuk mengetahui lebih lanjut fungsi functions yang saya pakai di sini, bolehlah rujuk kepada pandas API Reference.\nVisualisasi Data menggunakan Plotly Plotly adalah sebuah library yang menawarkan fungsi-fungsi untuk mempermudah pengguna untuk menghasilkan visualisasi interaktif. Ia ditawarkan dalam bahasa Python, R, ataupun JavaScript. Saya berpeluang untuk menggunakan Plotly dalam Python untuk menghasilkan visualisasi untuk thesis bachelor saya dan berdasarkan pengalaman saya, sangat mudah untuk bereksperimen dan menghasilkan graf visualisasi menarik menggunakan library ini.\nCiri Plotly yang sangat bagus adalah Plotly Express. Untuk kebanyakan fungsi visualisasi, Plotly Express sudah cukup pandai menakrif data yang diberikan dan kemudian menghasilkan visualisasi seperti yang dikehendaki. Berikut adalah code yang saya gunakan untuk menghasilkan animasi graf yang saya paparkan di permulaan blog post ini:\n1 2 3 4 5 6 7 8 9 fig = px.bar( state_data, x=\u0026#34;state\u0026#34;, y=[\u0026#34;partially_vaxed\u0026#34;, \u0026#34;fully_vaxed\u0026#34;], animation_frame=\u0026#34;date\u0026#34;, animation_group=\u0026#34;state\u0026#34;, labels={\u0026#34;value\u0026#34;: \u0026#34;Total vaccinated\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;variable\u0026#34;: \u0026#34;Dose Type\u0026#34;}, title=\u0026#34;Vaccination Count in Malaysia by State\u0026#34;, ) Jika anda perasan, saya hanyalah menggunakan satu function sahaja daripada Plotly Express iaitu bar. Function ini digunakan untuk menghasilkan visualisasi graf bar. Sebagai parameter, saya berikan data vaksinasi yang telah dibersihkan dan ditukarkan ke format DataFrame. Menerusi parameter x dan y, saya menetapkan data daripada column manakah dalam DataFrame tersebut yang akan digunakan sebagai paksi X dan paksi Y dalam graf.\nSeterusnya, untuk menghasilkan animasi bergerak, saya menggunakan parameter animation_frame dan ditetapkan column \u0026ldquo;date\u0026rdquo; sebagai nilainya (value). Dengan parameter ini, Plotly akan menghasilkan satu graf untuk setiap nilai dalam column tersebut. Jadi bila saya menggunakan column \u0026ldquo;date\u0026rdquo;, Plotly akan menghasilkan satu graf untuk setiap tarikh dalam data vaksinasi. Untuk menghasilkan animasi, graf-graf ini akan disusun mengikut tarikh dan dipaparkan seolah-olah slideshow. Hasil akhirnya kita akan dapat perkembangan kadar vaksinasi selaras dengan masa.\nParameter animation_frame cukup untuk menghasilkan animasi perkembangan kadar vaksinasi tersebut tetapi animasinya kelihation tidak begitu lancar dan seperti terpotong-potong. Oleh itu, saya juga menggunakan parameter animation_group. Dengan parameter ini, Plotly akan mencuba untuk melancarkan transisi antara dua graf yang dihasilkan berdasakan nilai column dalam animation_frame tadi. Dalam visualisasi graf bar, Plotly akan menunjukkan pertukaran posisi bar tersebut apabila ia berubah kedudukan. Dengan ini animasi kita tadi telah pun menjadi lebih lancar.\nAkhir sekali, parameter labels dan title digunakan untuk menetapkan label yang lebih mesra pembaca untuk legend, paksi, serta tajuk graf.\nKonklusi Saya amat berpuas hati dengan animasi graf ini kerana saya telah belajar cara untuk menghasilkan jenis bentuk graf yang telah saya minati buat sekian lama. Namun begitu, walaupun graf ini kelihatan lebih cantik berbanding graf lain dengan animasi bergerak, saya akui apa yang telah saya hasilkan ini lebih kepada latihan menggunakan library Plotly itu sendiri. Masih banyak aspek yang boleh diperbaiki untuk menyampaikan maklumat menggunakan graf secara tepat dan efektif.\nUntuk mengakses segala code yang telah saya tunjukkan di sini, boleh akses repository pokgak/citf-graphs di Github. Saya juga telah menetapkan jadual berkala supaya graf visualisasi tersebut dikemas kini setiap hari menggunakan Github Actions. Blog post cara saya bagaimana saya buat akan datang.\n","date":"Aug 25","permalink":"https://pokgak.xyz/articles/graf-interaktif-citf-plotly/","tags":null,"title":"Animasi interaktif berdasarkan data CITF menggunakan Plotly"},{"categories":null,"contents":"Github Actions (GA) adalah servis automation yang ditawarkan oleh Github untuk semua penggunanya. Jika anda mempunyai repository public di Github, anda boleh mula menggunakan Github Actions pada saat ini tanpa perlu membayar apa-apa pun!\nBagaimana untuk mula dengan Github Actions? Untuk mula menggunakan Github Actions, anda boleh pergi ke mana-mana repository public yang anda miliki dan seterusnya pergi ke tab Actions.\nJika anda belum pernah setup mana-mana workflow di repository tersebut, anda akan melihat pilihan templates siap yang boleh digunakan untuk pelbagai jenis projek. Sebagai pemula, saya cadangkan anda mula dengan template barebones yang ditawarkan.\nAnda boleh menggunakan editor local di komputer sendiri tapi Github juga ada menawarkan editor online di mana fail workflow anda akan diperiksa formatnya secara langsung sambil anda menaip. Github akan highlight jika fail workflow anda mempunyai kesalahan yang membuatkan workflow anda akan gagal. Selain itu juga, di tepi editor online itu ada dipaparkan documentation ringkas mengenai syntax fail workflow jadi anda tidak perlu lagi tukar-tukar tab untuk semasa menulis fail workflow anda.\nAnatomi fail workflow Github Actions Saya telah beberapa kali menyebut \u0026ldquo;fail workflow\u0026rdquo; dalam perenggan sebelum ini tapi belum pernah menerangkan apakah fail workflow itu. Github Actions menggunakan fail workflow untuk menetapkan bagaimana untuk melakukan automasi. Fail ini ditulis dalam format YAML. Satu ciri-ciri penting yang saya mahu highlight di sini adalah format YAML adalah whitespace-sensitive, bermakna anda perlu pastikan indentation fail workflow anda menggunakan 4 spaces.\nSebelum bermula, ini adalah isi akhir fail workflow contoh kita:\n1 2 3 4 5 6 7 8 9 10 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! - uses: actions/checkout@v2 Ikuti penjelasan saya di bawah untuk memahami apakah yang akan dilakukan apabila workflow ini dijalankan.\nKeyword dalam fail workflow Github Actions Dalam fail workflow anda ada dua top-level keyword yang wajib: on dan jobs.\nKeyword on Satu ciri-ciri penting Github Actions adalah, workflow anda perlu dimulakan melalui \u0026ldquo;triggers\u0026rdquo;. Hampir semua aktiviti yang anda boleh lakukan secara manual di Github boleh dijadikan trigger untuk workflow anda. Sebagai contoh, anda boleh menetapkan workflow untuk dijalankan apabila seseorang telah push codenya ke repo, atau apabila pull request baru dibuka. Ini cara bagaimana anda melakukan kedua-dua contoh tersebut:\n1 2 3 on: push: pull_request: Keyword on digunakan untuk menanda bahawa semua keyword dibawahnya adalah event-event dimana fail workflow anda patut dijalankan. push bermakna apabila seseorang telah push codenya ke repo anda, maka Github Actions akan menjalankan fail workflow tersebut. pull_request pula bermakna jika seseorang telah membuka pull request (PR) baru di repository anda, maka fail workflow tersebut akan dijalankan.\nKedua-dua keyword push dan pull_request ini juga boleh menerima sub-keyword lain untuk tujuan menapis dengan lebih spesifik bila workflow itu patut dijalankan. Antara sub-keyword yang boleh digunakan adalah branches untuk menapis hanya push atau pull request kepada branch yang dinyatakan sahaja. Anda juga boleh menapis mengikut lokasi fail code anda di dalam repo menggunakan sub-keyword paths.\nTerdapat banyak lagi keyword yang anda boleh gunakan untuk trigger workflow anda, jika berminat boleh pergi ke page ini dan ini untuk membaca lebih lanjut.\nKeyword jobs Okay, kita telah tetapkan bila workflow ini patut dijalankan menggunakan keyword on. Seterusnya kita akan menetapkan apa yang workflow ini patut buat menggunakan keyword jobs. Sesebuah workflow mestilah mempunyai paling kurang satu job. Untuk mencipta job baru, anda boleh menggunakan apa-apa perkataan sebagai id cuma perlu dipastikan tiada space. Contohnya seperti berikut:\n1 2 3 jobs: job-pertama: runs-on: ubuntu-latest Di sini, job-pertama adalah id untuk job kita. Seterusnya, setiap job perlulah menetapkan di bawah environment manakah job ini akan dijalankan. Github Actions menawarkan platform Windows, Linux, dan macOS yang anda boleh gunakan secara percuma. Senarai penuh versi yang disokong boleh dibaca di halaman ini. Di sini saya menggunakan ubuntu-latest yang bermakna, job ini akan dijalankan di platform Ubuntu yang terbaru (pada masa tulisan ini adalah Ubuntu 20.04.\nSetelah menetapkan platform, tiba masa untuk kita senaraikan apakah yang patut workflow kita ini buat. Untuk itu kita perlukan keyword steps. Seperti keyword jobs, keyword steps mengandungi sub-keywords yang, satu untuk setiap apa yang kita mahu jalankan.\nSetiap satu step akan dimulakan dengan simbol -. Dalam syntax YAML, ini menandakan bahawa semua keyword di bawah satu - adalah satu bahagian. Keyword run digunakan untuk menjalankan command seolah-olah anda berada di terminal platform yang telah dipilih menggunakan keyword runs-on sebelum ini.\n1 2 3 4 5 6 7 8 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! Dalam contoh di atas, saya telah menetapkan step itu untuk run command echo. Command ini akan print perkataan selepas itu ke terminal anda, dalam kes ini anda akan melihat \u0026ldquo;Hello, world\u0026rdquo; di log result workflow anda nanti. Dalam contoh di atas juga, saya telah menetapkan workflow ini untuk run command echo tapi kali ini dengan perkataan lain pula. Selain daripada keyword run, setiap step juga boleh ditetapkan dengan keyword-keyword lain seperti name, id dan pelbagai lagi. Senarai penuh boleh anda lihat di halaman ini. Fungsi simbol - di sini adalah untuk membantu mengumpul semua keyword yang berkaitan dengan step itu. Setiap simbol - bermakna satu step dalam job itu.\nKeyword uses Kita telah melihat bagaimana cara untuk menjalankan sebarang command melalui keyword run. Untuk sesetengah perkara, sekadar bergantung kepada command mungkin akan membataskan apa yang anda boleh lakukan. Oleh itu, Github Actions juga mempunyai fungsi untuk memanggil code luar dari fail workflow anda. Code ini boleh berasal dari repo yang sama ataupun daripada repo developer lain di Github.\nActions ini boleh ditulis dengan pelbagai cara sama ada menggunakan Javascript atau melalui Docker. Github juga menyediakan marketplace untuk anda mencari actions yang sesuai untuk digunakan dalam fail workflow anda. Github sendiri mempunyai beberapa Actions yang essential seperti checkout untuk checkout git repo anda sewaktu workflow dijalankan dan juga setup-node untuk setup environment node/javascript anda.\nUntuk menggunakan Actions, ada perlu menggunakan keyword uses diikuti dengan nama Actions yang ingin digunakan. Kebanyakan Actions juga mempunya keyword tersendiri yang digunakan untuk memperincikan bagaimana Actions tersebut dijalankan.\n1 2 3 4 5 6 7 8 9 10 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! - uses: actions/checkout@v2 Dalam contoh di atas, saya menggunakan Actions dari Github actions/checkout untuk melakukan git checkout repo saya ke sewaktu workflow dijalankan. @v2 di bahagian belakang itu menandakan versi Action tersebut yang ingin saya gunakan. Versi yang ditawarkan oleh Action tersebut boleh disemak di page Releases Action tersebut.\nKonklusi Saya pernah menggunakan Jenkins dan Bitbucket Pipeline dan berdasarkan pengalaman saya Github Actions adalah jauh lebih baik dari kedua-dua produk CI/CD tersebut. Dokumentasi Github Actions yang ditawarkan Github adalah sangat lengkap. Saya paling banyak merujuk halaman Workflow Syntax semasa mula belajar menggunakan Github Actions. Selain itu, halaman-halaman lain dalam Reference ini juga sangat membantu anda ingin mula melakukan perkara yang lebih advance dengan Github Actions.\nAntara contoh automation yang pernah saya lakukan menggunakan Github Actions adalah, menjalankan unit test untuk setiap commit push, memeriksa dan baiki tajuk pull request secara automatik jika tidak memenuhi kriteria anda. Saya juga pernah menggunakan Github Actions workflow untuk melakukan DB dump daripada server dan terus upload ke S3. Pada pandangan saya, Github Actions sangat menarik dan macam-macam yang anda boleh lakukan dengannya.\n","date":"Aug 24","permalink":"https://pokgak.xyz/articles/pengenalan-github-actions/","tags":["Github Actions","CI/CD"],"title":"Pengenalan Github Actions"},{"categories":null,"contents":"","date":"Nov 26","permalink":"https://pokgak.xyz/articles/","tags":null,"title":"Articles"}]