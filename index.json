[{"categories":null,"contents":"As part of my recent DDoS mitigation effort, I had to go through millions of nginx logs to identify patterns that I can use to further improve our custom WAF rules on Cloudflare. This article shows what I did to be able to run some analysis on the logs using DuckDB.\nMaking the hard things easy Changing the log format \u0026ldquo;To solve a problem that is difficult, you must first make it easy.\u0026rdquo;\nThe default nginx access logs format looks something like this:\n\u0026#39;$remote_addr - $remote_user [$time_local] \u0026#39;\u0026#39;\u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#39; \u0026#39;\u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34;\u0026#39; Parsing it is definitely possible using regex as shown by this article but why bother when you have the option to change the format and make it easier to parse using DuckDB. To do so we will be configuring ingress-nginx controller in our cluster to log in JSON format. You can set the custom log format using the log-format-upstream config and set log-format-escape-json to make sure that the variables are escaped properly for use in as JSON variables.\nThis is the log format that I\u0026rsquo;m currently using:\n\u0026#39;{\u0026#34;timestamp\u0026#34;: \u0026#34;$time_iso8601\u0026#34;, \u0026#34;requestID\u0026#34;: \u0026#34;$req_id\u0026#34;, \u0026#34;proxyUpstreamName\u0026#34;: \u0026#34;$proxy_upstream_name\u0026#34;, \u0026#34;proxyAlternativeUpstreamName\u0026#34;: \u0026#34;$proxy_alternative_upstream_name\u0026#34;,\u0026#34;upstreamStatus\u0026#34;: \u0026#34;$upstream_status\u0026#34;, \u0026#34;upstreamAddr\u0026#34;: \u0026#34;$upstream_addr\u0026#34;,\u0026#34;method\u0026#34;: \u0026#34;$request_method\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;$host\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;$uri\u0026#34;, \u0026#34;uriNormalized\u0026#34;: \u0026#34;$uri_normalized\u0026#34;, \u0026#34;uriWithParams\u0026#34;: \u0026#34;$request_uri\u0026#34;, \u0026#34;status\u0026#34;: $status,\u0026#34;requestSize\u0026#34;: \u0026#34;$request_length\u0026#34;, \u0026#34;responseSize\u0026#34;: \u0026#34;$upstream_response_length\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;$http_user_agent\u0026#34;, \u0026#34;remoteIp\u0026#34;: \u0026#34;$remote_addr\u0026#34;, \u0026#34;referer\u0026#34;: \u0026#34;$http_referer\u0026#34;, \u0026#34;latency\u0026#34;: \u0026#34;$upstream_response_time s\u0026#34;, \u0026#34;protocol\u0026#34;:\u0026#34;$server_protocol\u0026#34;}\u0026#39; Mapping customer IDs to generic placeholder One thing I don\u0026rsquo;t know how to do with DuckDB is normalizing URIs. Given a URI path containing user ID 12345678 like /users/1234567/info. How do I group by path where I ignore the middle section and group it as if all the URIs are like /users/:userId/info. I tried regex and patterns but couldn\u0026rsquo;t get it to work. If you know how to do it please DM me on Twitter, I\u0026rsquo;d really appreciate it.\nI found a way to do it in nginx instead. It works but its definitely not scalable. I use the ngx_http_map_module module to match URIs with certain paths and then convert it to a generic version of that path.\nmap $uri $uri_normalized { \u0026#34;~^/user/(.*)$\u0026#34; \u0026#34;/user/:ID\u0026#34;; default $uri; } This snippet will do the following: for each of the variable $uri, map it to a new variable $uri_normalized. When $uri value matches the regex ^/user/(.*)$, the replace the value with new value /users/ID. If no matching regex found, then use default to the value of $uri.\nWith those configured, you can proceed to ship the logs to your logging backend of choice to retrieve later.\nFetching the logs from Loki I\u0026rsquo;m using Loki as my logging backend. Loki provides an API endpoint you can use to fetch the logs and to make things easier they also have an CLI tool called logcli.\nTo fetch all nginx logs from my production cluster, this is the command that I used:\n$ logcli query -oraw --from=\u0026#34;2024-10-24T00:00:00+08:00\u0026#34; --to=\u0026#34;2024-10-24T22:00:00+08:00\u0026#34; --part-path-prefix=\u0026#34;logs\u0026#34; --parallel-max-workers=100 --parallel-duration=15m \u0026#39;{namespace=\u0026#34;ingress-nginx\u0026#34;, cluster=\u0026#34;production\u0026#34;} | json | __error__=``\u0026#39; -oraw is to set the output format of the logs. I\u0026rsquo;m using the raw format here so that I can get the same JSON input that I sent to Loki --from and --to are self-explanatory but I did have some problem specifying the correct format that the tool will accept and the official docs was quite confusing --part-path-prefix use this prefix to name the files when downloading multiple files in parallel --parallel-max-workers sets the max parallel workers to be used. Note that the actual workers used depends also on the available tasks based on the parallel duration configured --parallel-duration is the duration size to use for each file. combined with the --from and --to option, this option determines how many files will be created e.g. for 1 hour duration and you\u0026rsquo;ve specified the --parallel-duration there will be 4 files created, each containing logs from specific 15 minutes section. Loading the logs into DuckDB I\u0026rsquo;m using the CLI version of duckdb but you can also do this using the embedded library in other languages of your choice.\nTo read the files we\u0026rsquo;ve pulled from Loki and create a table with it I used this command:\ncreate table logs as select * from read_json(\u0026#39;logs_*.part\u0026#39;, format=\u0026#39;auto\u0026#39;, columns={timestamp: \u0026#39;TIMESTAMP\u0026#39;, method: \u0026#39;VARCHAR\u0026#39;, host: \u0026#39;VARCHAR\u0026#39;, uri: \u0026#39;VARCHAR\u0026#39;, uriNormalized: \u0026#39;VARCHAR\u0026#39;, status:\u0026#39;INT\u0026#39;, remoteIp: \u0026#39;VARCHAR\u0026#39;, is_authed: \u0026#39;BOOLEAN\u0026#39;, userAgent: \u0026#39;VARCHAR\u0026#39;}); read_json can automatically create all the columns based on the keys in the JSON files but I want it to treat certain columns as specific data types so that\u0026rsquo;s why I\u0026rsquo;m specifying the columns manually here.\nAfter loading the columns into the table you can check the number of rows using this commands:\nselect count() from logs; In my case, for a day\u0026rsquo;s worth of logs from nginx, I have around 60 million rows. On idle, its using around 8GBs of RAM. If you have longer periods of logs to analyze, then definitely opt for a beefier machine or else DuckDB will crash.\nRunning queries Its the fun part. Here\u0026rsquo;s some queries that I find useful:\nHighest requests per minute grouped by IP I use this information to set the proper value to use in our Cloudflare rate limiting rule. By looking at existing request rate I\u0026rsquo;m reducing the chance of rate limiting our actual customers.\nselect (hour(timestamp) + 8) % 24 as hour, minute(timestamp) as minute, remoteIp, count() as total from logs where remoteIp not in (\u0026#39;X.X.X.X\u0026#39;, \u0026#39;Y.Y.Y.Y\u0026#39;, \u0026#39;Z.Z.Z.Z\u0026#39;) --production NAT gateway IPs and remoteIp not like \u0026#39;162.158.192.%\u0026#39; --cloudflare IPs group by all order by total desc limit 5; IP address with the highest number of request to a particular host select remoteIp, count() as total from logs where host = \u0026#39;subdomain.example.com\u0026#39; group by all order by total desc limit 10; Checking the paths an IP have been sending requests to I\u0026rsquo;m including the result from the query here since it shows that this particular IP most likely are using a scanner to find vulnerable endpoints on our service.\nselect uri, count() as total from logs where remoteIp = \u0026#39;152.32.189.70\u0026#39; group by all order by total desc limit 20; ┌─────────────────────────────────────────────────────────────────┬───────┐ │ uri │ total │ │ varchar │ int64 │ ├─────────────────────────────────────────────────────────────────┼───────┤ │ / │ 10 │ │ /favicon.ico │ 6 │ │ /api/user/ismustmobile │ 6 │ │ /h5/ │ 4 │ │ /m/ │ 4 │ │ /api │ 4 │ │ /app/ │ 3 │ │ /api/config │ 3 │ │ /leftDao.php?callback=jQuery183016740860980352856_1604309800583 │ 2 │ │ /public/static/home/js/moblie/login.js │ 2 │ │ /static/home/css/feiqi-ee5401a8e6.css │ 2 │ │ /client/static/icon/hangqingicon.png │ 2 │ │ /admin/webadmin.php?mod=do\u0026amp;act=login │ 2 │ │ /static/images/auth/background.png │ 2 │ │ /index/index/home?business_id=1 │ 2 │ │ /stage-api/common/configKey/all │ 2 │ │ /Public/home/common/js/index.js │ 2 │ │ /ws/index/getTheLotteryInitList │ 2 │ │ /app/static/picture/star.png │ 2 │ │ /resource/home/js/common.js │ 2 │ ├─────────────────────────────────────────────────────────────────┴───────┤ │ 20 rows 2 columns │ └─────────────────────────────────────────────────────────────────────────┘ Conclusion This has been an adhoc task and when I was told to do some analysis on the logs I immediately think of the tools I\u0026rsquo;m familiar with which is DuckDB to do the analysis. I\u0026rsquo;m aware there are better tools out there and we are currently evaluating using OpenSearch Security Analytics to automatically do this kind of detection in the future. Hopefully, I can talk about it soon. If any of you data analyst/data engineer out there got better ways to do the things I\u0026rsquo;m doing feel free to tweet me @pokgak73 on Twitter.\n","date":"Oct 27","permalink":"https://pokgak.xyz/articles/duckdb-nginx-logs/","tags":["security","duckdb","analytics","ingress-nginx","k8s"],"title":"Using DuckDB to analyze NGINX logs"},{"categories":null,"contents":"Recently at $work we\u0026rsquo;ve been hit by a series of DDoS attacks. In this post, I\u0026rsquo;m gonna describe the steps we\u0026rsquo;ve taken to protect our services from these attacks in the future and also what works and what don\u0026rsquo;t.\nDetection The first attack was around 10PM on a Sunday. I was at home at the time and was notified that our ingress-nginx pods were repeatedly crashing. I\u0026rsquo;ve seen this happen before and my initial thought was that our service was getting more customers this night so I added more nodes into our cluster and increased the replica count for the ingress-nginx pods. That did nothing. Our pods are still crashing and customers still cannot use our service.\nThen, one of my colleagues showed me the metrics for new connections to our load balancer. We\u0026rsquo;re getting 10x our usual traffic in a minute. That\u0026rsquo;s a DDoS for sure.\nRate limiting from the ingress-controller Ingress-nginx supports a whole set of rate limiting features. So we decided to start there but we first need to know which endpoint is being hit. For this, the logs from the ingress-nginx pods was really helpful. The incoming logs was coming in fast but we can see that most of the log lines contain the same hostname. So we put the nginx.ingress.kubernetes.io/limit-rps annotation to the ingress that is used by that hostname. Then, we wait and monitor whether this annotation helps stabilize our crashing pods. It didn\u0026rsquo;t.\nFrom the logs, I can see that ingress-nginx is rate-limiting the request but because it still has to process the request first and keep counting it, it cannot keep up with the amount of incoming traffic. Rate limiting on the ingress-nginx level is useless because our infra is not enough to even receive and block all the request. We have an option to add more replicas to our ingress-nginx pods but thats gonna be costly. So, we started looking for other solutions.\nCloudflare to the rescue I\u0026rsquo;ve always have known that people uses Cloudflare to protect against it I\u0026rsquo;ve never done it myself personally. After the initial response was proven not effective, I remember we\u0026rsquo;re using Cloudflare to point the domain to our load balancer but why does Cloudflare not blocking all this DDoS traffic?\nOur first mistake was that we didn\u0026rsquo;t proxy the traffic through Cloudflare.\nAfter proxying the traffic through Cloudflare though, there is a delay to when all the traffic will be routed to Cloudflare Anycast IP due to the TTL settings which is 300 seconds by default. So, again, we wait\u0026hellip;and the request still coming in after a while. Meanwhile, I\u0026rsquo;ve enabled the Under Attack mode but honestly I\u0026rsquo;m not sure if this actually helps. After a while the requests was still not going down. It might take a while before Cloudflare can kick in to automatically mitigate the attack. In the meantime, we need to do something.\nUsing the WAF Rules On the WAF page on Cloudflare, there\u0026rsquo;s three types of rules you can configure: Custom rules, Rate limiting rules and Managed rules.\nManaged rules After enabling the Under Attack mode, we also enabled the Managed rules but it didn\u0026rsquo;t help much in our case. Managed rules block commonly used attacks but it our case it doesn\u0026rsquo;t block any of the DDoS attacks because the attack is targeting our API endpoint specifically with requests path that wasn\u0026rsquo;t included in the managed ruleset. We leave it turned on regardless since it might protect against other attacks in the future.\nCustom rules: blocking by country After all the above doesn\u0026rsquo;t seem to work, we need a way to differentiate DDoS traffic from valid traffic. I know that we only operate in several countries in Southeast Asia. This means that all the traffic that\u0026rsquo;s coming from outside those countries are bots (read more below to see why this is not true). So, we added a Custom rule to only whitelist the traffic coming from the countries that we operate in and that works, sorta. The request hitting our LB reduced to around half but its still higher than usual and our ingress pods are still being overwhelmed.\nThen, we noticed from the Security Analytics page in Cloudflare that the requests that are still hitting the LB passed the rule because its coming from one of the whitelisted countries. We can remove that country from our whitelist but that also means that we\u0026rsquo;ll be blocking valid traffic from our customers from those countries. So, we need to come up with new rule to block the DDoS traffic. What does a valid request have that the requests from the attacker doesn\u0026rsquo;t have?\nCustom rules: blocking using query params, headers, and user agent We went through nginx logs and noticed that the requests from the attacker are always using the same query params so we created a new rule blocking rqeuests with that query params and it worked! The requests hitting our LB dropped back to normal levels and we declared the incident finished. This doesn\u0026rsquo;t last long tho. The next time we were hit with the attack, we noticed that the attacker now uses a different query param. Luckily, we had also added other rules in place.\nAfter the first attack, we analysed our valid requests and came up with other rules based on the Referer and the X-Requested-With headers. We also check the User-Agent and block if its similar to the one that came from the attacker based on past attacks. So far this has been the most effective at blocking the attack. However, we know that this is not the final solution. If the attacker is determined enough, they can still look at valid requests and then spoof the values in their attack but so far we haven\u0026rsquo;t seen this happening yet.\nCustom rules: blocking known attacker IPs After several rounds of attacks we noticed that the DDoS attacks were all coming from the same set of IPs, so we created a list of known attacker IPs on Cloudflare and block future requests coming from those IPs. Looking back, this rule was only effective for a little while. Once the attacker noticed that all their requests were blocked, they will change the IP so the process will just keep repeating over and over again.\nRate limiting rule After we put in the custom rules, we also turned on the rate limiting rules. Unless you are on the Enterprise plan (its expensive XXXX), the rate limiting rule is pretty restricted. You can only rate limit by IP but I think it is good enough. The rate limiting rules will act as the last line of defense after the requests passes all your other configured custom rules. Here is the rule execution order for your reference.\nOn a free plan, you get 10 second counting period but if you pay for other plans you\u0026rsquo;ll get more options. To me, the bigger counting period helps prevent from blocking valid requests. There might be a burst of activity from your users that causes the IP to hit the rate limit within 10 seconds but if measured within a longer period its still within a normal range. So, paying more is definitely worth it here.\nI definitely think that rate limiting rule is a must if you\u0026rsquo;re fighting against DDoS. So, make sure to configure this.\nBonus Blocking our own IPs This is one of those facepalm moments in my life. After the attacks passed, I spent some time exploring the events in Cloudflare Security Analytics to see if I can find any insights. From the requests that were not blocked by Cloudflare, I grouped the requests by IP and checked the requests. Requests from the top two IPs looks good, it has all the headers and referers we were expecting them to have but the requests coming from a datacenter in Singapore. What makes it more suspicious was that all the requests had user agents from mobile devices eventhough the IP shows that they\u0026rsquo;re coming from a datacenter. So, I informed my team and proceed to block the requests. After a while complaints started coming in saying our customers requests were blocked. One of my colleagues suspects that those are actually our IPs.\nWe have NAT Gateways configured in our network which means that if the requests were actually from our own network it will have one of those IPs from the NAT Gateway\u0026hellip;and after comparing, they are indeed our NAT Gateway IPs. Apparently, one of the services proxies all the requests from the customers back to another service, complete with all the headers and user-agents that why it was showing mobile device user agents eventhough the IP was coming from a datacenter.\nAfter this incident, we created a list on Cloudflare containing all our known IPs and skip blocking to avoid confusion in the future.\nBlocking accessibility bots (from US) We also put in place rate limiting rule for all the requests that was categorized as Verified Bots by Cloudflare. After putting in all these rules, I try to regularly review the block requests to make sure they\u0026rsquo;re not false positives - valid requests that was blocked - to further optimize our rules. There was a bunch of requests coming from the US, which we already put in custom rule to block but after looking at their user agent it seems a bit unusual - it contains the string Google-Read-Aloud. After some googling, I found out that Google uses that user agent for their text-to-speech feature for accessibility purposes.\nHaving this user-agent does not necessarily mean that the requests are 100% valid because attackers can still spoof their user agent. So, I\u0026rsquo;ll leave it up to you to decide if this is something that should be blocked but I think this is worth mentioning since in our fervor to prevent attackers from bringing down our systems we might also be hurting valid customers and affecting the accessibility of our service.\nSilly mistake: external-dns Fast forward a few days, we got hit again with a DDoS attack but this time it was during lunch time, which was the peak hour for our customers. I thought, \u0026ldquo;Did the rule from last time not working anymore?\u0026rdquo;. We checked the Security Analytics page on Cloudflare and noticed that Cloudlfare wasn\u0026rsquo;t blocking any requests even though we already had the rule from last time.\nI scratched my head for a bit wondering what we missed when my colleague pointed out that the DNS record for that domain wasn\u0026rsquo;t proxied. So, I turned it back on but after a while it was turned back off. Then, I remember that we\u0026rsquo;re using external-dns to automate the creation of our records on Cloudflare and it was configured to disable the proxy option. So, whenever we enabled the proxy option, external-dns reverted it back as its supposed to. We end up turning off external-dns to make sure the proxy option would not get reverted. Read more below to know how you can turn on the proxy option on a per-ingress basis.\nUsing annotation to proxy traffic through Cloudflare per Ingress To enable proxying requests through Cloudflare on a per-Ingress basis when using external-dns, you can add the annotation external-dns.alpha.kubernetes.io/cloudflare-proxied: \u0026quot;true\u0026quot; to the Ingress. If you have multiple Ingress that all points to the same host, make sure to add the annotation to all of them. If not, external-dns will keep fighting itself, turning the proxy option onn and off forever.\nConclusion I\u0026rsquo;ve outlined several steps you can take if you\u0026rsquo;re facing DDoS attacks in the future. Despite the success in mitigating the attacks so far, we know that there is no forever solution to DDoS. We have to keep up with the attacker and play Whac-A-Mole until they are bored and stop the attacks. When fighting DDoS attacks, I find it helpful to log the requests and review it regularly to avoid false-positives. It has been an eye opening experience for me and next time I\u0026rsquo;m asked how to protect from a DDoS, I can definitely say more than just put it behind Cloudflare.\n","date":"Oct 27","permalink":"https://pokgak.xyz/articles/we-got-ddosed/","tags":["security","ddos","cloudflare","aws","ingress-nginx"],"title":"We got DDoSed"},{"categories":null,"contents":"I was having some issues figuring out how to scrape cAdvisor metrics using Grafana Alloy. After googling I came across this k8s-monitoring helm chart and inside there is a configuration for scraping the built-in cAdvisor on the k8s kubelet.\nI ran Alloy as a single pod Deployment and it\u0026rsquo;ll scrape all the nodes in the cluster. Here\u0026rsquo;s the config that I used to get the metrics:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 prometheus.remote_write \u0026#34;default\u0026#34; { endpoint { url = \u0026#34;https://mimir.example.com/api/v1/push\u0026#34; } } discovery.kubernetes \u0026#34;nodes\u0026#34; { role = \u0026#34;node\u0026#34; } discovery.relabel \u0026#34;cadvisor\u0026#34; { targets = discovery.kubernetes.nodes.targets rule { replacement = \u0026#34;/metrics/cadvisor\u0026#34; target_label = \u0026#34;__metrics_path__\u0026#34; } } prometheus.scrape \u0026#34;cadvisor\u0026#34; { job_name = \u0026#34;integrations/kubernetes/cadvisor\u0026#34; targets = discovery.relabel.cadvisor.output scheme = \u0026#34;https\u0026#34; scrape_interval = \u0026#34;60s\u0026#34; bearer_token_file = \u0026#34;/var/run/secrets/kubernetes.io/serviceaccount/token\u0026#34; tls_config { insecure_skip_verify = true } forward_to = [prometheus.remote_write.default.receiver] } Alloy cadvisor exporter Alloy provides the prometheus.exporter.cadvisor components that can be used to start a new cadvisor on the nodes. This is not required if the kubelet running on your nodes already runs cadvisor. This is the case for me on EKS running on Bottlerocket.\n","date":"Jul 10","permalink":"https://pokgak.xyz/articles/scrape-cadvisor-alloy/","tags":["grafana","alloy","cadvisor","observability","metrics"],"title":"Scrape cAdvisor using Grafana Alloy"},{"categories":null,"contents":"At my latest $job, I was tasked of setting up the LGTM stack (Loki, Grafana, Tempo, Mimir) for observability. Fast forward a few months, I noticed there\u0026rsquo;s a hidden aspect to running the stack that I was not expecting before and that is the network cost, specifically the network transfer cost for cross AZ traffic. At one point we were paying more than $100 per day just for the cross AZ network traffic.\nUpdate: since this article was written, I found out that the official Loki helm chart have a section addressing the cross-az issue in the values file. It does not recommend running Loki across multiple AZs on the cloud.\nNote: This can be used to run Loki over multiple cloud provider availability zones however this is not currently recommended as Loki is not optimized for this and cross zone network traffic costs can become extremely high extremely quickly. Even with zone awareness enabled, it is recommended to run Loki in a single availability zone.\nCross AZ Traffic Amplification While investigating where does the traffic coming from I compared the load balancer \u0026ldquo;Processed Bytes\u0026rdquo; metrics with the Cost Explorer usage for cross AZ traffic and noticed that there\u0026rsquo;s a 10x increase in the reported values by the load balancer to the actual charged traffic. It baffled me a bit and made me step back and take a deeper look at the possible points where I\u0026rsquo;m getting charged.\nCollector to load balancer node load balancer node to ingress controller pod ingress controller pod to distributor distributor to ingester Collector to Load Balancer Node: client routing policy In my setup, the services are exposed through a load balancer and given a DNS name like loki.example.com. The collectors are configured to send the telemetry data to that URL. Here is my fist mistake, I didnt\u0026rsquo; enable \u0026ldquo;Availability Zone affinity\u0026rdquo; for the client routing policy. When enabled, this will route traffic from the collector to the load balancer node in the same AZ avoiding being charged for cross AZ traffic.\nThe load balancer node will then forward the traffic to the ingress controller pod in the same AZ.\nIngress controller pod to distributor: Kubernetes Topology Aware Routing From the load balancer node, the traffic will be forwarded to the k8s pod through the k8s service. The default behavior of service in k8s is it will route the traffic using the round-robin algorithm. This means that from the ingress controller pod to the distributor pod the traffic will go cross AZ. If you have 3 distributor pods, this means 2 out of 3 connections will be routed to pods in different AZ.\nTo avoid the traffic from crossing AZ, we can use kubernetes topology aware routing feature. Downside of using this is that we need to have at least 3 pods in each AZ but compute is cheaper in my use case since I\u0026rsquo;m using spot instances through Karpenter and getting up to 70% discount on the node price.\nDistributor to Ingester: no workaround This the only part I haven\u0026rsquo;t solved. In the LGTM stack, the distributors uses an internal discovery mechanism to get the IP of the ingesters. This means that we cannot use kubernetes topology aware routing here.\nTo make things worse, depending on the replication_factor configuration, each distributor might be sending the logs to multiple ingesters, each one multiplying the cross AZ cost that we have to pay.\nSpecial use case: getting logs from external source Other than the above use case, our company also have other use case where the logs comes from external sources instead of from our internal network. In this case, I actually managed to eliminate the cross AZ cost completely by deploying the LGTM stack in just one AZ. The load balancer is also configured to use only one subnet that is in the same AZ.\nConclusion All the above factor might explain why the amount I was charged for cross AZ traffic is 10x bigger than the amount that is received at the load balancer. I outlined some the possible points where the cross AZ charges are coming from and how to fix it. Hope it helps!\n","date":"Jun 24","permalink":"https://pokgak.xyz/articles/hidden-cost-lgtm/","tags":["grafana","loki","mimir","tempo","observability","aws"],"title":"The hidden cost of running your own observability stack"},{"categories":null,"contents":"As a so called Tech Janitor, I\u0026rsquo;ve been tasked to clean up one of our AWS accounts at work and that account have a bunch of EC2 instances that no one knows what they all do. So, I\u0026rsquo;ve decided to use one of AWS features, VPC Flow Logs, to first identify which EC2 instances are still being used and which are not.\nSetting up the VPC Flow Logs and query using DuckDB For our purpose, I\u0026rsquo;ve setup VPC flow logs to send all the traffic data to a S3 bucket that we\u0026rsquo;ll refer to as vpc-flow-logs-bucket in this post. The flow logs are stored in a Parquet format for querying later using DuckDB.\nOnce the flow logs file are sent to S3, I\u0026rsquo;ll be able to query them using DuckDB. To do that we will need to install the aws and httpfs extensions.\nFrom DuckDB shell:\n\u0026gt; INSTALL aws; \u0026gt; INSTALL httpfs; We also need to load our AWS credentials into DuckDB. Luckily, DuckDB has a built-in command to do that:\n\u0026gt; CALL load_aws_credentials(); ┌──────────────────────┬──────────────────────────┬──────────────────────┬───────────────┐ │ loaded_access_key_id │ loaded_secret_access_key │ loaded_session_token │ loaded_region │ │ varchar │ varchar │ varchar │ varchar │ ├──────────────────────┼──────────────────────────┼──────────────────────┼───────────────┤ │ \u0026lt;redacted\u0026gt; │ \u0026lt;redacted\u0026gt; │ │ eu-west-1 │ └──────────────────────┴──────────────────────────┴──────────────────────┴───────────────┘ This will look for your AWS credentials based on the standard AWS credentials file location. If you have multiple profiles in your credentials file, you can specify which profile to use by passing the profile name as an argument to the load_aws_credentials function.\nNow it\u0026rsquo;s time to load our VPC flow logs from S3 into a table in DuckDB. You can replace the year/month/day/hour with the actual date and hour of the flow logs that you want to load or use * for any or all of them to load all the flow logs. I\u0026rsquo;ll be loading all the flow log records into a table flow_logs in DuckDB.\nThis might take a while since DuckDB will have to download the Parquet files from S3 and load them into memory. It took several minutes to finish loading in my case.\n\u0026gt; CREATE TABLE flow_logs AS SELECT * from read_parquet(\u0026#39;s3://vpc-flow-logs-bucket/AWSLogs/\u0026lt;aws-account-id\u0026gt;/vpcflowlogs/\u0026lt;region\u0026gt;/\u0026lt;year\u0026gt;/\u0026lt;month\u0026gt;/\u0026lt;day\u0026gt;/\u0026lt;hour\u0026gt;/*.parquet\u0026#39;) Now we can see that the flow logs records only contains the network interface ID (ENI) of the EC2 instance but not the EC2 instance ID or name itself. That won\u0026rsquo;t be enough for my use case since I want to identify which traffic is flowing to which EC2 instance. Therefore, we need to correlate the ENI with the EC2 instance ID and here\u0026rsquo;s where Steampipe comes in.\nSteampipe: directly query your APIs from SQL Steampipe is a tool that allows you to query APIs from SQL. It supports a lot of different APIs from AWS, GCP, Azure, Github, etc. You can also write your own plugins to support other APIs. I\u0026rsquo;ll be using it to query my AWS account for the EC2 instance ID and name based on the ENI ID from the VPC flow logs.\nLife before Steampipe Usually to do the things I\u0026rsquo;m about to show below, I\u0026rsquo;ll pull the data from AWS using the aws-cli and then massage it using jq/yq/awk/sed, if I\u0026rsquo;m desperate maybe Python. Then I\u0026rsquo;ll use some other tools to visualize it or export to CSV. With Steampipe, pulling the data from AWS is so simple and using SQL to correlate the data with other information source is a breeze.\nSteampipe is just Postgresql Under the hood, Steampipe is running PostgreSQL and it even allows you to run it as a standalone instance running in the background and allows connecting to it from any third-party tools that can connect to a Postgresql instance. Here\u0026rsquo;s where it gets interesting, DuckDB has the capability to connect to any PostgreSQL database through the Postgresql extension and query it as if all the data inside that database is coming from the DuckDB. This means that we can use Steampipe as a data source for DuckDB and access all of the AWS resources data available in Steampipe.\nSetting up Steampipe and DuckDB connection To run Steampipe as a service mode, you\u0026rsquo;ll need to run the following command to start the PostgreSQL instance and get the credentials for connecting to it:\n$ steampipe service start Database: Host(s): 127.0.0.1, ::1, 2606:4700:110:8818:e17b:f78c:6c52:dccb, 172.16.0.2, 2001:f40:909:8e2:207a:634a:2070:d99d, 2001:f40:909:8e2:1cdb:75da:2a70:4b05, 192.168.100.23, 127.0.2.3, 127.0.2.2 Port: 9193 Database: steampipe User: steampipe Password: ********* [use --show-password to reveal] Connection string: postgres://steampipe@127.0.0.1:9193/steampipe Then inside DuckDB shell, you can connect to the Steampipe PostgreSQL instance using the following command:\n\u0026gt; ATTACH \u0026#39;dbname=steampipe user=steampipe password=23e2_4853_bd96 host=127.0.0.1 port=9193\u0026#39; AS steampipe (TYPE postgres); \u0026gt; use steampipe.aws; \u0026gt; SHOW tables; show tables; ┌─────────────────────────────────────────────┐ │ name │ │ varchar │ ├─────────────────────────────────────────────┤ │ aws_accessanalyzer_analyzer │ │ aws_account │ │ aws_account_alternate_contact │ ... Now we can see all the tables from the Steampipe Postgresql instance. For my use case I\u0026rsquo;ll be using the aws_ec2_network_interface table which contains both the network interface ID (ENI) and the EC2 instance ID that I can use JOIN together with the VPC flow logs records to map the records to the EC2 instance ID.\nJOIN-ing it all together Here\u0026rsquo;s an example query that will give me the count of all incoming traffic to the instances grouped by the port number:\nselect i.title, fl.dstport, count(fl.dstaddr) traffic from aws_ec2_network_interface ni left join memory.flow_logs fl on fl.interface_id = ni.network_interface_id left join aws_ec2_instance i on i.instance_id = ni.attached_instance_id where fl.dstaddr = i.private_ip_address group by i.instance_name, fl.dstport order by traffic desc, dstport asc From this information I\u0026rsquo;ll be able to guess which service is running on those instances and take the next step towards migrating or depecrating the instances.\nConclusion It is kinda mindblowing that I can do all this using SQL. Both Steampipe and DuckDB are great products and the flexibility of those tools allows me to pick and choose the best tool for the job. I first came across Steampipe in one of the podcasts that I listen to but haven\u0026rsquo;t really used it much. Now, after having the opportunity to use it to solve one of my problems, I\u0026rsquo;ll definitely pay more attention to it to make my tech janitor life easier in the future ;)\n","date":"Jan 26","permalink":"https://pokgak.xyz/articles/steampipe-duckdb-flow-logs/","tags":["duckdb","steampipe","aws","cloud","tools","data","sql"],"title":"Using Steampipe + DuckDB for VPC Flow Logs Analysis"},{"categories":null,"contents":" Modules are containers for multiple resources that are used together.\nTerraform modules is a way to bundle a bunch of Terraform resources into one group. Although not explicitly mentioned in the definition, it is also a way to provide an abstraction to the resources inside the module and only expose inputs and outputs that are relevant to the users of the module.\nTerraform Module as an Abstraction Layer A Terraform resource usually tends to be generic in that it allows you to configure it multiple ways through the input variables that it can accept. For example, the aws_mskconnect_connector resource has 3 options for log delivery: CloudWatch Logs, Kinesis Data Firehose, or S3. In most cases, your module shouldn\u0026rsquo;t expose all 3 options to your users. Your organization probably already has some standard place where you send you logs to e.g. S3 for example where it gets forwarded to another logs search service for later use.\nTherefore, your module should only expose the S3 option and leave out CloudWatch Logs and Kinesis Data Firehose from your module. By doing this you eliminate the choice from the user and they don\u0026rsquo;t have to think which one to use. Your Terraform code can be a lot simpler too.\nModule Abstraction in Action Here\u0026rsquo;s an example module code when including all 3 options:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 variable \u0026#34;log_delivery_s3\u0026#34; { type = object({ bucket_arn = string prefix = optional(string, \u0026#34;mskconnect-logs\u0026#34;) }) default = null } variable \u0026#34;log_delivery_cloudwatch_logs\u0026#34; { type = object({ log_group = string }) default = null } variable \u0026#34;log_delivery_firehose\u0026#34; { type = object({ delivery_stream = string }) default = null } resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; log_delivery { worker_log_delivery { dynamic \u0026#34;s3\u0026#34; { for_each = var.log_delivery_s3 != null ? [1] : [] enabled = true bucket_arn = var.log_delivery_s3.bucket_arn prefix = var.log_delivery_s3.prefix } dynamic \u0026#34;cloudwatch_logs\u0026#34; { for_each = var.log_delivery_cloudwatch_logs != null ? [1] : [] enabled = true log_group = var.log_delivery_cloudwatch_logs.log_group } dynamic \u0026#34;firehose\u0026#34; { for_each = var.log_delivery_firehose != null ? [1] : [] enabled = true delivery_stream = var.log_delivery_firehose.delivery_stream } } } } And here\u0026rsquo;s an example when we only offer S3 option:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 variable \u0026#34;log_delivery_s3\u0026#34; { type = object({ bucket_arn = string prefix = optional(string, \u0026#34;mskconnect-logs\u0026#34;) }) default = null } resource \u0026#34;aws_mskconnect_connector\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example\u0026#34; log_delivery { worker_log_delivery { s3 { enabled = var.log_delivery_s3 != null bucket_arn = var.log_delivery_s3.bucket_arn prefix = var.log_delivery_s3.prefix } } } } See how simple and shorter the code become? No need to use those dynamic blocks just to conditionally add or remove the log delivery option block anymore.\nBut I might need to use that other option in the future\u0026hellip; YAGNI.\n","date":"Dec 08","permalink":"https://pokgak.xyz/articles/terraform-modules-opinionated/","tags":["opinions","terraform","design"],"title":"Terraform modules: be opinionated"},{"categories":null,"contents":"\nI recently setup a local kubernetes in my home network to play with and one of the issues that I faced is that it is hard to access the services inside the cluster from my laptop. I don\u0026rsquo;t have a load-balancer in my setup so everytime I want to access a service from my laptop, I\u0026rsquo;ll have to run kubectl port-forward first before using the localhost address to access it. It works but it\u0026rsquo;s annoying.\nUsually in cloud environments like AWS, you would setup an ingress-controller that will provision a load balancer for you and use that load balancer to expose your services inside the cluster to the internet using Ingress resources. Your incoming traffic from the internet will then be routed through the load balancer into your cluster onto your pods. Unfortunately, you don\u0026rsquo;t get the same thing when hosting your cluster locally outside of the cloud environment. You have to manually configure your network to allow access from the internet.\nSo what options do we have? One way we can do it is to use a Service with type NodePort to use the host port and access the pods using the host IP, this will allow access to services inside the cluster to your local network but still not from the internet. To allow access from the internet, you\u0026rsquo;ll have to open a port on your router to route to the host IP from the NodePort service.\nI\u0026rsquo;m not a fan of opening my home network to the internet. Home routers is infamous for being vulnerable and easily exploitable. I don\u0026rsquo;t want mine to be part of a new legion of botnets that will break a new record for biggest DDoS attack. Using the NodePort service type also is not that great. With NodePort service, you\u0026rsquo;ll have to specify the node IP to access along with the port assigned and your traffic will always go to that node and the pods running on it. More reason on why NodePort is a bad idea on StackOverflow.\nWhat other option do we have? Tailscale!\nTailscale Subnet Router Tailscale is a mesh VPN built on top of Wireguard. I\u0026rsquo;ve been using it for a long time for accessing my personal servers at home while I\u0026rsquo;m outside and I love it. It is so simple to setup you don\u0026rsquo;t have to know any networking magic to use it. Tailscale will create a peer-to-peer network from your client to your other Tailscale devices and it is also really smart in figuring out a way to punch a hole through your home network (see the Resources section) to connect to the internet so you don\u0026rsquo;t have to open a port on your home router anymore. Bot legion problem solved!\nOne of the ways you can use Tailscale is by configuring a Tailscale node as a subnet router. Usually, when you have 10 devices in your network, you\u0026rsquo;ll have to install Tailscale on each of those devices to connect it to your VPN network but with a subnet router only one Tailscale node in that network is enough, as long as that subnet router node have network access to all the devices in that network. You\u0026rsquo;ll have to configure your subnet router to advertise the route of the internal cluster network that the subnet router is in using CIDR range e.g. 10.43.0.0/16 so that other devices outside of that network will know to look for the subnet router if they want to access the IP address from that CIDR range.\nELI5: subnet router advertisement You\u0026rsquo;re a postman trying to deliver a parcel. Your parcel destination is set to unit A-1-2-3 in the TRX Exchange 106 building. You\u0026rsquo;ve never been to TRX before so you don\u0026rsquo;t know which floor the office actually is but you noticed there\u0026rsquo;s a big signboard at the reception saying \u0026ldquo;Come here if you have parcel for unit A-1-0-0 to A-9-9-9\u0026rdquo;. So, you went the reception and then the nice lady at the reception gave you the direction to reach the office unit A-1-2-3 for you to deliver your parcel.\nThe reception here is like our subnet router. All the traffic meant for the network have to go through the subnet router first, then they\u0026rsquo;re passed through to the actual packet destination.\nI don\u0026rsquo;t want to remember all this IP addresses With a subnet router, you can now reach any of the services inside the cluster using the ClusterIP of that service but IP address is not human-friendly and you don\u0026rsquo;t want to (you can\u0026rsquo;t!) memorize all the IP addresses for all the services inside the cluster. So, now we need something that will map our IP addresses to a human-friendly format. Sounds familiar? We can use DNS records.\nYou can definitely create DNS records manually and map it to each of the ClusterIP for your services. That\u0026rsquo;s what I did for testing when validating this setup actually. At scale, that won\u0026rsquo;t work tho. You don\u0026rsquo;t want to be the one to manually go to your DNS registrar and create the records one by one. Luckily, in kubernetes there is an application called external-dns.\nexternal-dns to the rescue external-dns is an application that runs inside your kubernetes cluster and it periodically queries the kubernetes API for the list of all Service and Ingress resources. From the list, it checks whether it should create a DNS record for the resources based on the resource annotation. It supports a lot of DNS providers like AWS Route53, Cloudflare, Google Cloud DNS and more. For my setup I\u0026rsquo;m using Cloudflare.\nBy default, external-dns will only create DNS records for Ingress resource or Service with type LoadBalancer. For my setup, since I\u0026rsquo;m self-hosting the cluster inside my home network and don\u0026rsquo;t have access to a load balancer, I have to add an extra configuration parameter to external-dns so that it will create DNS records for ClusterIP Service type. On the Service resource itself, usually external-dns searches for the external-dns.alpha.kubernetes.io/hostname annotation but since we\u0026rsquo;re using it with ClusterIP, I have to change it to external-dns.alpha.kubernetes.io/internal-hostname.\nTailscale + external-dns = ❤️ ➜ dig prometheus.k8s.pokgak.xyz +short 10.43.170.163 With those changes applied. All new Service resources in my kubernetes cluster that have the annotation will get one DNS record on Cloudflare. Now, if I try to resolve a name for a service inside the cluster, it will return me an internal ClusterIP. Combined with the Tailscale subnet-router we\u0026rsquo;ve configured earlier, now you can access services inside your cluster from any of your Tailscale devices from any part of the world.\nWith tailscale, you\u0026rsquo;ll also have an additional layer of authentication. Only users in your Tailscale networks can access the exposed services. For others, they might be able to guess what you have running in your cluster from your DNS records but they won\u0026rsquo;t be able to access it since all the IPs will be private IPs.\nFor the next part, I\u0026rsquo;m looking into exposing some service inside my cluster to the internet fully without having to be in the Tailscale network. Tailscale Funnel suppose to do just that but I still haven\u0026rsquo;t tested if it\u0026rsquo;s working with services inside kubernetes.\nResources How Tailscale Works: explanation on how Tailscale uses Wireguard to create a mesh VPN network architecture. How NAT traversal works: recommended read even if you\u0026rsquo;re not a networking geek. You\u0026rsquo;ll learn a thing or two about networking for sure. Full cofiguration for external-dns helm chart external-dns annotation for the services Tailscale subnet router inside kubernetes k8s manifest for my subnet router ","date":"Sep 12","permalink":"https://pokgak.xyz/articles/k8s-cluster-access-tailscale/","tags":["kubernetes","tailscale","external-dns","networking"],"title":"Access internal kubernetes services from anywhere using Tailscale"},{"categories":null,"contents":"\nI had the chance to give a talk at my local DevOps meetup last July and recently, did another one at a Tech Talk event hosted by my current employer. Both talks are related to OPA but this blog post will be more of a personal reflection for me just to document these moments in my career.\nThere are two common things that I noticed about myself from giving the talk and I\u0026rsquo;m gonna describe them here.\nThe First Common Thing Firstly, on days leading to the event I would slowly get agitated, the realness of having to do that talk creeps on me. I tend to procrastinate when faced with a big assignment like this. I will be so productive and do everything from playing a new game, thinking about a new project to do, cleaning my house, cooking, and eating \u0026mdash; except preparing and finishing the slides for the talk.\nOn the day of the talk itself, my productivity will be out of the window. I\u0026rsquo;d be reading, or doing something else but the thought of having to do the talk later is always on the back of my head. A few hours before the talk, I\u0026rsquo;m still polishing my slides, making last-minute changes, and going through the slides a couple more times in my head but around two hours before, I start distracting myself and doing other stuff. I\u0026rsquo;ll go talk to my colleagues, read up on unrelated topics, and even get the time to catch up on my novels. This is when things are getting real for me.\nUsually at these events, there will be food served before the event starts. For the first one, they had pizza and for the second event, they served rice with chicken curry and some kuih. They look really good tbh but I definitely won\u0026rsquo;t be touching those. I\u0026rsquo;m already struggling enough to keep my nerves together and the thought of having a taste of the food at the back of my throat when talking later will throw me into a full-blown panic mode. It\u0026rsquo;s like my sense is heightened and I get so sensitive to my surrounding that even keeping a conversation with others made me feel overwhelmed. My best companion at this moment is a bottle of mineral water \u0026mdash; tasteless, and I can fiddle with the bottle cap to distract myself.\nIt might feel like I\u0026rsquo;m exaggerating here but I can tell you that me before and after finishing the talk is so different I feel like we\u0026rsquo;re a totally different person.\nThe Second Common Thing As the host of the event invited me to the stage, I\u0026rsquo;ll bring my laptop, set it up, and connect it to the projector. Then, holding the mic, I\u0026rsquo;ll take a moment to look over the audience and take a deep breath.\nOnce I start talking, it\u0026rsquo;s like a switch flipped and all that nervousness is gone.\nJust like how I\u0026rsquo;d practiced, I would go through the slides, following the flow that I\u0026rsquo;ve set when putting the slides together. I like my presentation to have a story. I think it is more engaging to the audience and it is also a lot easier for me to remember what to say. I probably need to work on my tempo still (I blazed through 40+ slides in 20+ minutes 😆) but at least I wasn\u0026rsquo;t too nervous and blanked out mid-speaking. In the most recent talk that I gave, I even managed to slip in a joke during my introduction. Quite proud of that one. Hah!\nWhat\u0026rsquo;s the magic? I\u0026rsquo;m not sure if there\u0026rsquo;s any science behind this but one factor that I think contributed to this flip in the switch is my confidence in the topic. Before giving my first talk, I\u0026rsquo;ve been reading, thinking, and playing with the technology for about a year on and off. I won\u0026rsquo;t say that I mastered the topic but at least for the topic that I\u0026rsquo;m sharing, I feel like I have something to give to that the audience can benefit.\nFor my second talk, I\u0026rsquo;ve been involved in the development process from day one \u0026mdash; together with my team members, of course. So, at this point, I can say that I know the topic quite well.\nIt\u0026rsquo;s a journey My journey with public speaking started a long time ago. I used to be so nervous waiting for my turn to introduce myself to the class on the first day of school. Having all the attention on me even for a brief moment when I barged in the conversation with my friend group during our lepak session at mamak can make my ears turn red.\nMy first time speaking in front of a large audience was in a high school public speaking competition when I was 13. I was chosen because my English was pretty good in the class, not because I was good at public speaking, mind you. Being the inexperienced public speaker that I am, I printed my speech text on a big A4 paper and brought it to the stage, holding it full like that, not folding the paper whatsoever.\nI didn\u0026rsquo;t manage to memorize the whole thing so I kept looking at the text during my speech. After I finished, my friends told me they noticed how nervous I was from how the A4 paper in my hand was shaking so badly during the whole speech. I\u0026rsquo;ll always remember that one.\nFinal Note I can gladly say that I\u0026rsquo;ve improved a lot since then. Being exposed to these situations countless times made me realize that it is natural to feel nervous and I know that I\u0026rsquo;ll come out better after going through it.\nFor those who might struggle with the same issue, just know that others are going through the same thing too and most importantly, take the risk and put yourself out there and eventually you\u0026rsquo;ll get to the point where you\u0026rsquo;ll overcome that fear.\nSpecial thanks to the organizers for giving me the chance to put myself out there and my colleagues and friends for supporting me.\n","date":"Sep 08","permalink":"https://pokgak.xyz/articles/on-public-speaking/","tags":["personal","public speaking"],"title":"On Public Speaking"},{"categories":null,"contents":"This will be the first in my interview questions series. I\u0026rsquo;ll compile interesting questions that I got from my experience interviewing for DevOps/SRE role in Malaysia.\nCalling a service by its cluster-internal DNS We\u0026rsquo;ll go from the highest to the lowest level in this journey. So let\u0026rsquo;s go through the scenario a bit: you have two services, foo and bar. those two services live in the same namespace app in your cluster. Now, inside service foo code, it makes a HTTP request to service bar. Probably something like so:\nhttp.get(\u0026#34;https://bar/\u0026#34;) What happens behind the scene from when the request is made to service bar and until the response is received back by service foo?\nWhat is that weird DNS format? You might\u0026rsquo;ve noticed that we\u0026rsquo;re just calling the service bar by the name using a weird name. Instead of the usual something.com domain, we\u0026rsquo;re just using bar directly. How is this possible?\nKubernetes allows you to call other services by using the service resource name directly. It does this by automatically appending the full DNS domain to the given service name. So for example here, when you make a request to bar, the application will make a DNS request to the local DNS server. The DNS server then notices that the domain that it received is not \u0026ldquo;complete\u0026rdquo; so it automatically appends the rest of the domain name based on the configuration that was given to it. If the service is running inside the namespace app, it will turn bar into bar.app.svc.cluster.local.\nThis automatic appending to complete the domain name is called \u0026ldquo;search domain\u0026rdquo;. In our example the seach domain is configured as app.svc.cluster.local. So, whenever the service makes a call to bar it will automatically try to append the search domain and tries to resolve the domain name.\nHow (and where) is this configured? Every pods in kubernetes has a file /etc/resolv.conf that is configured by the kubelet when starting the pod. This file will contain the info where to find the DNS server inside the cluster and also what to use as the search domain. Here\u0026rsquo;s an example of the file (source):\nnameserver 10.32.0.10 search \u0026lt;namespace\u0026gt;.svc.cluster.local svc.cluster.local cluster.local options ndots:5 Which IP will be returned by the DNS query? The DNS query will return us a virtual service IP. Why virtual? It\u0026rsquo;s because this IP doesn\u0026rsquo;t actually points to a pod that runs our services.\nIn kubernetes, pods can come and go at any time which also means that their IP will change all the time. How do we know then where to send our requests to? The Service resource is used to abstract dynamic nature of pod IPs and provide a consistent IP that your application can use to send requests to it.\nHow does the service IP maps to pod IPs? The Service resource always comes with its pair, the Endpoint (or EndpointSlice) resource. This Endpoint resource tracks the pod IPs and also have information which pod IP is ready to receive traffic. This information can be queried using the kubernetes API.\nOn the node where the pod runs, there is a program called kube-proxy that runs and updates the routing to map from service IP to pod IP. This routing can be done in multiple ways but currently the default is using iptables.\nWhen does this routing happens? When a request is first sent from the application code, its destination will be set to the service IP but before the request is sent out over the network, iptables modifies the destination and changes the service IP to pod IP. If there are multiple pods that sits behind a service, the pod IP will be load balanced using a round-robin. Once the destination IP is changed, the packet is then sent out over the network.\nHow do you know which node to send the packet to? A kubernetes cluster can contain a lot of nodes. Sending the packet to the correct node is important. To know which node to send the packet to, the router in your network will need to know which node to send this packet to. If you setup your own cluster ala kubernetes-the-hard-way, you might need to configure these routes yourself but if you\u0026rsquo;re using kubernetes on top of any cloud providers, they usually will do these setup for you and you don\u0026rsquo;t have to do anything here.\nOnce that is sorted, your packet now can reach the correct node and the packet is sent to the correct pod on the node based on the destination pod IP set in the packet header. The response then will be sent to the source pod IP in the request packet header.\nResponse now sent back to the source node. All done? Not yet. There\u0026rsquo;s one more last thing to do. Remember when we sent the request originally, iptables had rewrote the destination from service IP to pod IP? Now for the response packet to be received back by the pod, the pod IP that we rewrote before needs to be converted back to the service IP.\nThis is needed because as far as the application knows, it sends a request to the service IP and not the pod IP. If it suddenly receives a response from a pod IP that it doesn\u0026rsquo;t know of, then it will just drop the response. So, here iptable will have to remember what it did before and convert pod IP on the response packet back to service IP. Finally, our foo service can receive the response that it wants from the bar service.\n","date":"May 24","permalink":"https://pokgak.xyz/articles/explain-k8s-dns/","tags":["interview","kubernetes","dns","networking"],"title":"Interview Series: Explain How Kubernetes DNS works"},{"categories":null,"contents":" Why? Why not? Get the whole picture of what is happening in your pipeline. Get notified when something is taking longer than it should.\nHow? Use otel-cli, a standalone Go binary that can create OpenTelemetry traces and sends to a tracing backend using the OTLP protocol.\nOpenTelemetry? https://opentelemetry.io/\nTracing Backend? You collect traces from your application using the OpenTelemetry SDK. To visualize the relationship between the traces, you\u0026rsquo;ll have to send the traces to a tracing backend, which will provide a UI for exploring your traces. Example of tracing backend:\nSelf-hosted:\nGrafana Tempo Jaeger ElasticSearch Paid:\nHoneycomb Datadog Grafana Cloud ElasticSearch Cloud OTLP Protocol The OpenTelemetry Protocol (OTLP) specification describes the encoding, transport, and delivery mechanism of telemetry data between telemetry sources, intermediate nodes such as collectors and telemetry backends.\nhttps://opentelemetry.io/docs/reference/specification/protocol/otlp/\notel-cli? OpenTelemetry (OTel) supports many SDK to create traces from your application but in CI pipelines, you\u0026rsquo;re usually using a shell script language like Bash which is not supported by any OTel SDKs currently. Therefore, we need a tool create this traces for us.\notel-cli is a tool that will do that. It will generate a trace ID, span ID, and sends the traces in the expected format.\nHow to use otel-cli? The simplest way to start using it is first to set the OTEL_EXPORTER_OTLP_ENDPOINT value to tell otel-cli which backend to send our traces to.\nStarting a local tracing backend server otel-cli has a server subcommand that you can use to run a simple tracing backend on your local. You can run the following command in another terminatl to start the server:\notel-cli server tui Setting the tracing backend endpoint Now that we have a server running locally to send our traces to, let\u0026rsquo;s tell otel-cli to send all the traces that it generated to this local server:\nexport OTEL_EXPORTER_OTLP_ENDPOINT=localhost:4317 Here we send it to localhost on port 4317. Port 4317 is the default port when sending traces using grpc.\nSending our first trace You can use exec subcommand to wrap a command with otel-cli. It will automatically set the start and end time to calculate the run duration for the command:\notel-cli exec --service my-service --name \u0026#34;My First Trace\u0026#34; echo \u0026#34;HELLO WORLD\u0026#34; Then you should be able to see the a new line in the other terminal that we ran otel-cli server tui just now.\nConclusion In this article, I showed you the simplest way you can use otel-cli. To get more valuable information from your traces, you\u0026rsquo;ll usually need to add nested spans to your trace. It\u0026rsquo;ll help break down the execution of your program to more smaller unit that can be inspected. To get more advanced example, you should refer to the otel-cli examples.\n","date":"Apr 08","permalink":"https://pokgak.xyz/articles/instrument-your-ci/","tags":["opentelemetry","otel","tracing","ci"],"title":"Instrumenting CI Pipelines using otel-cli"},{"categories":null,"contents":"Atlantis is an application used for collaborating on a Terraform code base using pull requests and one of the feature that it has is to run conftest and test a set of defined OPA policies. At the moment I\u0026rsquo;m writing this article, Atlantis only supports using local sources i.e. local filesystem as the source of the policy. In this article, I\u0026rsquo;ll show an example of how to use an S3 bucket instead as the source for the policies.\nCustom workflow and run step Atlantis supports using custom workflows to override the default commands that it runs and as part of that feature, it supports defining any custom commands to run as part of the steps for each stage. We will be abusingusing this feature to override the default conftest command that Atlantis uses and specify our policy through the --update flag of conftest\nconftest --update flag By using --update you can tell conftest to pull the policy first every time it wants to run the tests. We will be using an S3 bucket as our source but before we can pull from S3, you have to make sure that wherever the Atlantis server is running, it can access and have permission to pull objects from the bucket. In my case, Atlantis is running as a StatefulSet inside a Kubernetes cluster so I have already configured the IAM permission needed for it to access the bucket.\nconftest is using the go-getter package underneath to pull these packages so technically it should be possible to also pull from other sources that go-getter supports, other than just S3.\nResult Combining both of the features described above, here\u0026rsquo;s an example of a simplified repo config that I use:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # minimal config for brevity; you might need to configure more options to make atlantis works properly repos: - id: github.com/$ORG/$REPO workflow: custom workflows: custom: policy_check: steps: - show # important don\u0026#39;t skip this step - run: conftest test $SHOWFILE --update s3::https://s3-us-east-1.amazonaws.com/$BUCKET_NAME/policy policies: policy_sets: - name: policy-from-s3 path: /home/atlantis/policy source: local In the example above, under the workflows key, I\u0026rsquo;m defining a custom workflow named custom and inside that custom workflow, I\u0026rsquo;m overriding the default policy_check steps with my own. My custom policy_check steps consists of the show step and the custom run step. The show step is crucial since this is when Atlantis will run terraform show to convert your Terraform planfile to a JSON formatted file.\nWhen using the custom run step, Atlantis will store the path to this JSON formatted file in variable $SHOWFILE so when I ran my conftest command you can see that I\u0026rsquo;m using $SHOWFILE to run conftest against the file. Optional: if you want to run conftest against the Terraform files too, you can add *.tf after $SHOWFILE and it will include all the *tf files in that project directory.\nNext comes the --update flag, to specify the S3 bucket, I\u0026rsquo;m using a URL format that is specified by the go-getter package replacing $BUCKET_NAME with the bucket name that I have configured with the correct permission and network access. Inside the S3 bucket, this is how I structured the files. I put all the OPA policies inside a folder policy since conftest complains when I just put all the policies directly at the root level inside the bucket. YMMV.\n$BUCKET_NAME/ ├─ policy/ │ ├─ stop_it.rego │ ├─ dont_kill_server.rego After defining our custom workflow, we can specify the custom worklow as the default workflow for a repo. This is done by setting the repos[].workflow value to the name of our custom workflow, in my case it\u0026rsquo;s custom.\nNext, as part of the using the policy check feature in Atlantis, you are required to set the policies values. You can refer to the docs for the full configuration required. Inside the policies key, there is a required policy_check key that is used to specify where Atlantis can find the OPA policies to use when running conftest. Usually, this is a folder on a local filesystem already containing the policies but in our case, since we\u0026rsquo;re using the --update flag, we just need to specify any folder on the local filesystem that will be writable by the Atlantis user. You can see in the example above that I\u0026rsquo;m using /home/atlantis/policy.\nConclusion That\u0026rsquo;s all you need to do configure to make Atlantis pulls policies from S3 (and include Terraform source code files in your contest run). Shoutout to a DoorDash engineering blog post which mentioned briefly that they pulled their policies from S3 and made me curios how to do the same using Atlantis. You can mention me on Twitter (@pokgak73) if this article has helped you. That would most definitely made my day :)\n","date":"Nov 16","permalink":"https://pokgak.xyz/articles/conftest-pull-from-s3/","tags":["atlantis","opa","conftest","policy","s3","terraform"],"title":"How to use OPA policy from an S3 bucket when using Atlantis policy check"},{"categories":null,"contents":"I started using OPA at my $dayJob recently and there are some parts that I think is not intuitive to grok for beginners.\nIf you want to play around with the rules and input, you can use the Rego Playground. It\u0026rsquo;s super useful and I used it to test out policies and test my hypothesis when playing around with Rego language.\nHow does OPA, Rego, and conftest related to each other? Rego is a declarative language used to write OPA policies. Then, OPA is the engine that takes in the policies written in Rego and evaluates it, producing a set of documents called \u0026ldquo;rules\u0026rdquo;. You can use OPA and the Rego language directly to write policies for your config files but using conftest will make the DX much better. conftest builds on top of OPA and provide some extra functionality that makes using OPA easier.\nRego Basics The Rego language focuses on querying the input to look for a given condition. If the input satisfies the query, then it will produce the document.\nVariable Assignments Variable assignment in Rego works the same like in other language. The expression foo := \u0026quot;hello\u0026quot; will assign the value \u0026quot;hello\u0026quot; to the variable foo.\nOne difference in Rego is that it implicitly assigns value true to the document if the condition given evaluates to true. In the example below, there\u0026rsquo;s two ways to write the Rego expression. Rego actually implicitly assigns the value true so we can also remove\nfoo := \u0026#34;hello\u0026#34; # first way: explicitly assigns `true` to `result` when condition is satisfied result := true if foo == \u0026#34;hello\u0026#34; # Rego implicitly assigns `true` to `result` when condition is satisfied result if foo == \u0026#34;hello\u0026#34; Let\u0026rsquo;s bring this up to next level. Most of the time, the condition you\u0026rsquo;re checking is not as straight forward as checking the value against a static value. You might also need to evaluate expressions in between and save the intermediary values in a variable to help improve readability. In previous example we only used a one-liner for the rule body but you can also have more complex rule body like the following using curly braces.\nDeclarative Rego Language The Rego language is declarative and useful to query data structures for any value. Consider the following example (Rego playground link):\nLet\u0026rsquo;s assume our input is an array of object, each containing the keys \u0026ldquo;id\u0026rdquo; and \u0026ldquo;name\u0026rdquo;. In this policy we\u0026rsquo;re checking that the objects doesn\u0026rsquo;t have any forbiden value for \u0026ldquo;name\u0026rdquo;.\nforbidden_names := [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden if input.users[i].name == forbidden_names[j] This code would look something like this in Python:\n1 2 3 4 5 6 7 8 9 users = [{\u0026#34;name\u0026#34;: \u0026#34;foo\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;bar\u0026#34;}] forbidden_names = [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden = [] for i in range(len(input.users): for j in range(len(forbidden_names)): user_forbidden.push(input.users[i].name == forbidden_names[j]) return any(user_forbidden) For both codes, user_forbidden will evaluate to true if one of the user name is included in the forbidden_names list. In the Python code, we used for loops with the any() function to check that none of the value is true. In the Rego code, we don\u0026rsquo;t have to use any for loop or iterate through the user list. forbidden_names[i] means \u0026ldquo;for any of the values in forbidden_names. So in our Rego code, we essentially tells OPA, if any of the value in input.users is the same as any of the value in forbidden_name, then return set the value of user_forbidden to true.\nIn this case, since we are not using the index i and j to reference the value at those index anywhere in the policy, we can simplify it more by using _ (underscore) instead for the index. _ is like a throwaway value and we don\u0026rsquo;t care about the index, we just care if one of the values is the same in user.input and forbidden_names.\nuser_forbidden if input.users[_].name == forbidden_names[_] More complex policies Before this our policies are all simple one liner but Rego also supports writing the rule body in multiple lines. In the example below, we are adding an exception to the rule that the previous rule doesn\u0026rsquo;t apply to user with id == 5. So if one our user name value is john but have id == 5 then user_forbidden won\u0026rsquo;t evaluate to true. Note that we are using the same index i when accessing the name and id property. This means we are referring to the same user. If we use _ or a different index when accessing the name and id, the rule will evaluate to true.\nIf any of the expressions inside the rule body evaluates to false or undefined then it will stop evaluating the rule body and return undefined for user_forbidden.\nforbidden_names := [\u0026#34;foobar\u0026#34;, \u0026#34;john\u0026#34;] user_forbidden if { input.users[i].name == forbidden_names[_] input.users[i].id != 5 false print(\u0026#34;this will not be printed\u0026#34;) } Using conftest Previously, we used arbitrary names for our rules but conftest introduces a few keywords that we must use so that it can detect any failed rules and includes it in the output. Conftest will pick up any rules with name deny, warn, or violation and the summary will be shown in conftest output.\n➜ tree conftest conftest ├── input.json └── policy └── names.rego # input.json { \u0026#34;users\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;john\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;bar\u0026#34; }, { \u0026#34;id\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;foobar\u0026#34; } ] } # policy/names.rego package main import future.keywords.contains import future.keywords.if deny contains msg if { forbidden_names := [\u0026#34;john\u0026#34;] name := input.users[_].name name == forbidden_names[_] msg := sprintf(\u0026#34;username %v is not allowed\u0026#34;, [name]) } warn contains msg if { id := input.users[_].id id == 2 msg := sprintf(\u0026#34;id %v is not allowed\u0026#34;, [id]) } Run conftest against our input file:\n➜ conftest test input.json --policy policy/ WARN - input.json - main - id 2 is not allowed FAIL - input.json - main - username john is not allowed 2 tests, 0 passed, 0 warnings, 2 failures, 0 exceptions Note the values output here, the deny rule will be output as FAIL if the rule passes while the warn rule is counted as WARN. Here, conftest takes the output values from the OPA engine and formats the output for us to make it easier to interpret or integrate with other tools. You can also change the output format of conftest by passing in the --output flag. I like the github output since it will automatically prints the output in a format that Github Actions understoods and will surface error in Github UI approriately. You can also output it as JSON, which is great if you want to process the result output using tools like jq.\n➜ conftest test --help [...] -o, --output string Output format for conftest results - valid options are: [stdout json tap table junit github] (default \u0026#34;stdout\u0026#34;) JSON output:\n➜ conftest test input.json --output json [ { \u0026#34;filename\u0026#34;: \u0026#34;input.json\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;successes\u0026#34;: 0, \u0026#34;warnings\u0026#34;: [ { \u0026#34;msg\u0026#34;: \u0026#34;id 2 is not allowed\u0026#34; } ], \u0026#34;failures\u0026#34;: [ { \u0026#34;msg\u0026#34;: \u0026#34;username john is not allowed\u0026#34; } ] } ] parsers: using other format as input files Until now all our input has been in JSON format but conftest also has built-in parsers that can automatically detect the input format and converts it to JSON for us. As of this moment, here is the list of valid parsers: [cue dockerfile edn hcl1 hcl2 hocon ignore ini json jsonnet properties spdx toml vcl xml yaml dotenv].\nExample is for HCL2 code used for Terraform:\n# input.tf resource \u0026#34;aws_imaginary_resource\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;this\u0026#34; instance_type = \u0026#34;r5.4xlarge\u0026#34; security_groups = [\u0026#34;12345\u0026#34;, \u0026#34;45678\u0026#34;] } resource \u0026#34;aws_imaginary_resource\u0026#34; \u0026#34;that\u0026#34; { name = \u0026#34;that\u0026#34; instance_type = \u0026#34;t3.medium\u0026#34; ingress { port = 1234 cidr = [\u0026#34;0.0.0.0/0\u0026#34;] } } We can use conftest parse to see how conftest will parse the Terraform file and then write our policy based on the parsed input.\n➜ conftest parse input.tf { \u0026#34;resource\u0026#34;: { \u0026#34;aws_imaginary_resource\u0026#34;: { \u0026#34;that\u0026#34;: { \u0026#34;ingress\u0026#34;: { \u0026#34;cidr\u0026#34;: [ \u0026#34;0.0.0.0/0\u0026#34; ], \u0026#34;port\u0026#34;: 1234 }, \u0026#34;instance_type\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;that\u0026#34; }, \u0026#34;this\u0026#34;: { \u0026#34;instance_type\u0026#34;: \u0026#34;r5.4xlarge\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;this\u0026#34;, \u0026#34;security_groups\u0026#34;: [ \u0026#34;12345\u0026#34;, \u0026#34;45678\u0026#34; ] } } } } Conclusion I just got started with OPA but considering the flexibility of it when used with conftest, I feel like you can use this for a lot of use cases. The ability to separate the policy logic from your application is powerful and the declarative nature of the Rego language also helps simplify the policy a lot as demonstrated in my comparison of the Python code above.\n","date":"Nov 12","permalink":"https://pokgak.xyz/articles/opa-conftest/","tags":["opa","conftest","policy","terraform","rego"],"title":"Getting started with OPA and conftest"},{"categories":null,"contents":"Note: I\u0026rsquo;m using pseudocode in the code example in this article to keep the article brief. Please refer to the official Slack and OpenTelemetry documentation for the actual code.\nI\u0026rsquo;ve talked about the basics of OpenTelemetry in my previous article. In this one, I\u0026rsquo;ll explain more on how we\u0026rsquo;re integrating OpenTelemetry with our Slack-based application.\nAt the end of this article, this is roughly how the span lifetime and events created will look like:\nSlack BoltJS Socket Mode Compared to the a standard HTTP request/response, we\u0026rsquo;re using BoltJS with socket mode. This gives us the advantage of not having the application exposed publicly to be able to accept requests from Slack but this also means that we cannot just use the auto-instrumentation for HTTP developed by the community.\nSocket mode uses WebSocket to establish connection to Slack and exchange messages through that connection. There is no official auto-instrumentation support for the ws library that is used by BoltJS socket mode but I found opentelemetry-instrumentation-ws, a 3rd-party library for ws library auto-instrumentation.\nSpent a few days integrating it into our application and in the end I concluded that the auto-instrumentation provided by the opentelemetry-instrumentation-ws is too low-level. Our goal is to track user interactions with the application - when they use the bot, which option they choose, what were they trying to do, and whether the interaction ends successfully or with an error. The library, however, created spans when a new connection is established between our application and Slack but no spans or events for user interactions.\nSo, the conclusion? We\u0026rsquo;ll instrument the application manually.\nCreating and Ending Spans Since this application is used company-wide, it\u0026rsquo;s highly likely that multiple users will be using it in parallel. To track user interactions independent from each other, we\u0026rsquo;ll also need separate spans for each user.\nI decided to go with an object spanStore storing the user spans. Like a singleton pattern, a new span will be created for that user if it doesn\u0026rsquo;t exist yet in spanStore, otherwise it will just return the existing user span.\n1 2 3 4 5 6 7 8 9 10 11 12 spanStore = {} function getUserSpan(username) { if (user in spanStore) { return spanStore[username] } span = startSpan(username) spanStore[username] = span return span } Now that we have a function to create the span, when in the lifetime of the incoming event do we create the span? Ideally, as early as possible before anything else so that we can track everything. BoltJS supports setting a global middleware that will be called before the event handler function are called. This is where I call the getUserSpan() function above. For the first event for that user, it will create a new span and for the next events it will just return the existing spans that I can use.\nNext, when do you end the span? Due to how the application works, we\u0026rsquo;re assuming that each user can only have one session at one time and at the end there will always be an finishing event triggered when the user finished their interaction with the application. Based on that fact, I wrote an event listener that will respond to this finishing event by calling the OTel function to end the span and remove the span from the spanStore object above.\n1 2 3 4 5 app.event({id: \u0026#39;finishing_event\u0026#39;}, async ({username}) =\u0026gt; { span = getUserSpan(username) span.end() removeSpanFromSpanStore(span) }) Tracking User Actions with Span Events With these, we have a separate span for each user for the whole duration of their interaction with the application. With only one span, we don\u0026rsquo;t have insights yet into what the user are doing, which actions are taken by the user, so we\u0026rsquo;ll need a way to track user actions.\nWith Slack BoltJS, we can trigger a listener function on every user interaction. I wrote a function that will create a new span event using the user input id as the event name. I also passed in the whole payload so that the we can see the payload of user actions later when debugging issues. Add this as another global middleware, now we\u0026rsquo;re creating a new event for every user actions.\n1 2 3 4 app.action(\u0026#39;callback_id\u0026#39;, async ({username, action_id, payload}) =\u0026gt; { span = getUserSpan(username) span.addEvent(action_id, {payload}) }) Confession I\u0026rsquo;m actually not convinced that my way of doing this is correct. One of the reason is that since I use one root span for the whole interaction for a user, I\u0026rsquo;m also tracking the duration taken by the user to do the next action. From our perpective, this made the duration of the span tracked is now kinda useless for us since it also includes factors that are not controllable by us (time taken for users to do the next action).\nInstead of one root span and creating new span events for every user interaction, maybe a new span for each interaction, linked to the previous span would be better since we only track the duration that we are in control of, not how long the user takes to click a button.\nNevertheless, since I already implemented like this now, let\u0026rsquo;s see how that will turn out. Like the saying, you either die a hero, or you live long enough to see yourself become the villain.\n","date":"Aug 13","permalink":"https://pokgak.xyz/articles/otel-slack-integration/","tags":["Observability","otel-slack"],"title":"Instrumenting a Slack bot with OpenTelemetry"},{"categories":null,"contents":"I got to work on integrating OpenTelemetry in an application that our team maintains recently so I\u0026rsquo;m starting a series documenting my learnings throughout this journey.\nA little background info on the application I\u0026rsquo;m working on: it\u0026rsquo;s a Slack chatbot written in Typescript using BoltJS. Our goal is to know how many users are using our Slack bot with a breakdown of the percentage of successful and error interactions. When an error happened, we also want to know what exactly the user did and the current state of the application that caused it to error. Based on my reading, the last sentence is exactly what observability promises, So that\u0026rsquo;s why we\u0026rsquo;re giving it a try.\nOpenTelemetry can be divided into three categories: tracing, logging, and metrics; but I\u0026rsquo;ll be focusing on tracing in this series.\nTracing Primers To get started you should know some basic concepts about tracing.\nTraces, Spans A trace consists of multiple spans and a span is a unit of work with a start and end time. In a span, you can create events that marks when something happened in the lifetime of the span.\nA span can also have nested spans and these are called child spans. The parent span is usually representing some abstract unit of work, like the lifetime of a HTTP request when it from when it hits the application until the response is sent. Child spans can be used to get more details into the operations done during the lifetime of that parent span ie. API call to another service to fetch more informations.\nSpan attributes, Status, Errors To add context to the spans, you can set custom attributes. Ideally, you want to send all the information that will help when debugging your application in the future so that later you don\u0026rsquo;t have to modify the code and add more attribute when you noticed an issue and realized that you don\u0026rsquo;t have enough information to debug the issue.\nIf your application encounters an error, you can set the span status to ERROR and also add the stack trace to the context for use in debugging. By default your span status will be set to OK.\nSpan Exporter After the span ends, you\u0026rsquo;ll want to send it to a backend service that will store and process it so that you can use it later. The sending is done by OTel Exporters. There are multiple backend available that accepts OTel traces as inputs but such as Jaeger, Zipkin but for my testing I\u0026rsquo;m using Honeycomb with the OLTP Collector.\nDebugging For debugging, there\u0026rsquo;s also the ConsoleSpanExporter which will print out your spans in the console instead of sending it anywhere. I find this very useful to get fast response on what is being sent over but it\u0026rsquo;s hard to do analysis with it so in production environment you should configure the exporter to use other backends instead.\nAutomatic vs Manual Instrumentation Now we got the basics out of the way, let\u0026rsquo;s look at how you can start adding spans to your application to build traces.\nThe easiest way to get started is to use auto instrumentation which will automatically injects code in the HTTP, requests, DNS, libraries that you\u0026rsquo;re using to create spans and events. In nodejs, this can be done by installing the auto-instrumentations-node NPM package. This package pulls in several other packages to automatically instrument your application.\nThis is a nice onboarding experience but I get overwhelmed by the amount of data sent when by these auto instrumentation package. Therefore, I recommend to you to start with manual instrumentation instead.\nWith manual instrumentation, you\u0026rsquo;re forced to be intentional with the data that you\u0026rsquo;re sending to the backend. With this I get to decide which information I want to send over and already have in mind what I want to do with it and which information I would like to gain from it.\nInitialization Whatever approach you end up with for the instrumentation, you\u0026rsquo;ll want to make sure that you\u0026rsquo;re initializing the OTel libraries at the start of your application. This is required because if you starts it later, your application might already be handling request when your OTel libraries are not initialized yet, causing it to miss some requests, or worse encounter errors.\nThe recommended way to do it is to use the -r flag from the node command:\n-r, \u0026ndash;require module Preload the specified module at startup. Follows require()\u0026rsquo;s module resolution rules. module may be either a path to a file, or a Node.js module name.\nSo in your package.json you\u0026rsquo;ll have to add that to your start command:\n1 2 3 scripts: { \u0026#34;start\u0026#34;: \u0026#34;node -r ./tracing.js app.js\u0026#34;, } If you\u0026rsquo;re using Typescript like me, you\u0026rsquo;ll want to use the NODE_OPTIONS shell variable to specify the flag instead:\n1 2 3 scripts: { \u0026#34;start\u0026#34;: \u0026#34;NODE_OPTIONS=\u0026#39;-r ./tracing.js\u0026#39; ts-node app.ts\u0026#34;, } NodeSDK vs NodeTracerProvider Confusion One thing that made me confused is how different the code for initializing auto instrumentation compared to manual instrumentation.\nThis is the code provided by Honeycomb to use auto instrumentation. The key there is the getNodeAutoInstrumentation() function which will register all the supported auto instrumentation libraries. One more thing is that it is using the NodeSDK class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // tracing.js (\u0026#34;use strict\u0026#34;); const { NodeSDK } = require(\u0026#34;@opentelemetry/sdk-node\u0026#34;); const { getNodeAutoInstrumentations } = require(\u0026#34;@opentelemetry/auto-instrumentations-node\u0026#34;); const { OTLPTraceExporter } = require(\u0026#34;@opentelemetry/exporter-trace-otlp-proto\u0026#34;); // The Trace Exporter exports the data to Honeycomb and uses // the environment variables for endpoint, service name, and API Key. const traceExporter = new OTLPTraceExporter(); const sdk = new NodeSDK({ traceExporter, instrumentations: [getNodeAutoInstrumentations()] }); sdk.start() On the other hand, this is the code example from opentelemetry.io to start manual instrumentation. Notice that it\u0026rsquo;s not using the NodeSDK class anymore and you need to create the Resource and NodeTracerProvider objects and configure it yourself.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 const opentelemetry = require(\u0026#34;@opentelemetry/api\u0026#34;); const { Resource } = require(\u0026#34;@opentelemetry/resources\u0026#34;); const { SemanticResourceAttributes } = require(\u0026#34;@opentelemetry/semantic-conventions\u0026#34;); const { NodeTracerProvider } = require(\u0026#34;@opentelemetry/sdk-trace-node\u0026#34;); const { registerInstrumentations } = require(\u0026#34;@opentelemetry/instrumentation\u0026#34;); const { ConsoleSpanExporter, BatchSpanProcessor } = require(\u0026#34;@opentelemetry/sdk-trace-base\u0026#34;); // Optionally register automatic instrumentation libraries registerInstrumentations({ instrumentations: [], }); const resource = Resource.default().merge( new Resource({ [SemanticResourceAttributes.SERVICE_NAME]: \u0026#34;service-name-here\u0026#34;, [SemanticResourceAttributes.SERVICE_VERSION]: \u0026#34;0.1.0\u0026#34;, }) ); const provider = new NodeTracerProvider({ resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor); provider.register(); TBH I\u0026rsquo;m still not clear what is the difference betwen using NodeSDK vs manually configuring the NodeTracerProvider. When using NodeSDK does the NodeTracerProvider got configured automatically?\nHow and when to start tracing? To start manually instrumenting your application, you\u0026rsquo;ll have to create a root span. A root span is the first span you create once the request enters your application.\nNow, if you have a normal HTTP request/response-based application, it is easy to figure out where to start and end your root spans. All your incoming requests will most likely be handled by a controller and each endpoint will be handled by a method. In this type of application, your root span can be started once the request hits the application in the method in your controller and ends before you send the response.\nDuring the lifetime of that request, you can create child spans to track other works done while processing the request. There\u0026rsquo;s only one entry point for requests and exiting the entry point means the request is finished. If your application encountered errors during the execution, it can set the span status to ERROR and add the stack trace info to the span.\nConclusion Once you managed to create spans, set attributes, and then export it to a backend. You\u0026rsquo;re pretty much done with the basics of instrumenting your application. Go ahead and add more traces to your application!\n","date":"Aug 13","permalink":"https://pokgak.xyz/articles/otel-basics/","tags":["observability","opentelemetry","otel-slack","honeycomb"],"title":"OpenTelemetry Basics"},{"categories":null,"contents":"HCL adalah bahasa yang digunakan dalam produk-produk daripada Hashicorp seperti Terraform dan Packer. Kebiasaannya, fail HCL ini ditulis secara manual tetapi jika anda ingin menulis atau mengubah fail-fail tersebut secara programmatik menggunakan code, maka anda boleh menggunakan hclwrite, sebuah library yang ditulis dalam Go.\nBlog post ini dibahagikan kepada dua bahagian. Bahagian pertama menunjukkan cara untuk menghasilkan block baru from scratch dan simpan ke fail. Ini adalah asas untuk bahagian kedua di mana kita akan mengubah fail HCL sedia ada dan memastikan format fail tersebut terjaga dan tidak melakukan pengubahan secara semberono.\nSaya tidak akan memberi penerangan penuh syntax fail HCL kerana ia boleh didapati di halaman ini.\nBahagian 1: Cipta block baru daripada mula Untuk bahagian 1 ini, kita akan belajar cara untuk:\ncipta block baru tambah attribute dalam block tersebut simpan block yang dicipta ke dalam fail Untuk contoh pertama, kita akan cuba generate block HCL di bawah:\n1 2 3 4 resource \u0026#34;github_membership\u0026#34; \u0026#34;user\u0026#34; { username = \u0026#34;github_username\u0026#34; role = \u0026#34;member\u0026#34; } Inilah code yang diperlukan untuk generate block tersebut:\n1 2 3 4 5 6 7 8 9 newMemberBlock := hclwrite.NewBlock(\u0026#34;resource\u0026#34;, []string{\u0026#34;github_membership\u0026#34;, mlId}) body := newMemberBlock.Body() body.SetAttributeValue(\u0026#34;username\u0026#34;, cty.StringVal(githubUsername)) body.SetAttributeValue(\u0026#34;role\u0026#34;, cty.StringVal(\u0026#34;members\u0026#34;)) f := hclwrite.NewEmptyFile() f.Body().AppendBlock(newMemberBlock) f.Body().AppendNewline() ioutil.WriteFile(\u0026#34;data/result_members.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Cipta block Mula-mula kita perlukan sebuah block untuk mengisi content-content lain ke dalamnya. Ini boleh dicipta menggunakan function hclwrite.NewBlock(). Parameter pertama function ini adalah nama type, kemudian diikuti dengan label-label bagi block tersebut. Dalam contoh block di atas, nama type yang kita perlukan adalah \u0026ldquo;resource\u0026rdquo; dan kita memerlukan label \u0026ldquo;github_membership\u0026rdquo; dan \u0026ldquo;user\u0026rdquo;.\nSeterusnya kita boleh mula mengisi boleh yang baru sahaja kita cipta tadi. Dalam contoh di atas, block itu mengandungi attribute \u0026ldquo;username\u0026rdquo; dan \u0026ldquo;role\u0026rdquo; dengan nilai masing-masing. Kita boleh set attribute sesebuah block dengan function SetAttributeValue().\nUntuk nama attribute, kita boleh menggunakan string biasa tetapi bagi nilai attribute tersebut, hclwrite menggunakan library cty (sebut: si-tai) untuk memastikan nilai attribute tersebut mempunyai type yang betul setelah habis proces pemprosesan nanti. Bagi memasukkan nilai string menggunakan library cty, kita boleh menggunakan function cty.StringVal(), yang akan menukarkan string Go biasa kepada nilai cty yang setaraf.\nSimpan block ke dalam fail 1 2 3 4 f := hclwrite.NewEmptyFile() f.Body().AppendBlock(newMemberBlock) f.Body().AppendNewline() ioutil.WriteFile(\u0026#34;data/result_members.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Dengan itu selesai bahagian pertama iaitu mencipta block tersebut menggunakan code. Seterusnya, kita perlu menyimpan block yang telah kita cipta ini ke dalam fail. Untuk memudahkan, kali ini kita akan bermula dengan fail baru yang kosong. Untuk bermula dengan fail kosong, kita boleh menggunakan function hclwrite.NewEmptyFile(). Fuction ini seolah-olah memberi kita kanvas kosong untuk kita isikan dengan block-block yang akan kita reka.\nUntuk menambah block ke fail tersebut, kita tidak boleh menambahnya terus ke objek File yang dipulangkan oleh function NewEmptyFile. Semua content dalam sebuah fail perlu diletakkan dalam bahagian Body block tersebut. Kita boleh mengakses Body melalui function Body().\nSeterusnya, kita boleh tambah block yang telah kita siapkan dalam bahagian sebelum ini menggunakan function ApppendBlock ke dalam Body yang telah dapat dalam langkah sebelum ini. Untuk memastikan block kita itu nampak kemas, maka kita boleh tambah baris kosong di hujung fail dengan menggunakan function AppendNewLine.\nAkhirnya, untuk menyimpan semua yang telah kita generate ini ke fail, kita boleh menggunakan function ioutil.WriteFile(). Kita boleh memasukkan content fail kita dengan cara menukarkannya kepada bytes. hclwrite juga mempunyai function Format untuk memastikan fail yang telah kita cipta itu mematuhi recommended format untuk sesebuah fail HCL. Selepas itu anda bolehlah menyemak fail HCL yang dihasilkan di lokasi yang telah diberi semasa memanggil function WriteFile tadi.\nBahagian 2: Mengubah block sedia ada Untuk bahagian 2 ini, kita akan belajar cara untuk:\nbaca dan parse fail HCL sedia ada cari bahagian untuk kita ubah tambah pengubahan yang diinginkan menggunakan Token beza Traversal dan Value Fail yang ingin kita hasilkan adalah seperti berikut:\n1 2 3 4 5 6 7 8 9 10 module \u0026#34;team_itsm_team\u0026#34; { source = \u0026#34;../../modules/github/team_nx\u0026#34; team_name = \u0026#34;ITSM Team\u0026#34; members = [ github_membership.kasan.username, github_membership.mismail.username, // *kita ingin menambah baris ini ] } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 content, _ := ioutil.ReadFile(\u0026#34;data/\u0026#34; + pod + \u0026#34;.tf\u0026#34;) f, _ := hclwrite.ParseConfig(content, \u0026#34;\u0026#34;, hcl.InitialPos) block := f.Body().FirstMatchingBlock(\u0026#34;module\u0026#34;, []string{\u0026#34;team_\u0026#34; + pod + \u0026#34;_team\u0026#34;}) oldMembers := block.Body().GetAttribute(\u0026#34;members\u0026#34;).Expr().BuildTokens(nil) newEntry := hclwrite.NewExpressionAbsTraversal( hcl.Traversal{ hcl.TraverseRoot{Name: \u0026#34;github_membership\u0026#34;}, hcl.TraverseAttr{Name: mlId}, hcl.TraverseAttr{Name: \u0026#34;username\u0026#34;}, }, ).BuildTokens(nil) newMembers := append( oldMembers[:len(oldMembers)-2], \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, ) newMembers = append(newMembers, newEntry...) newMembers = append(newMembers, hclwrite.Tokens{ \u0026amp;hclwrite.Token{Type: hclsyntax.TokenComma, Bytes: []byte{\u0026#39;,\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte{\u0026#39;]\u0026#39;}}, }...) block.Body().SetAttributeRaw(\u0026#34;members\u0026#34;, newMembers) ioutil.WriteFile(\u0026#34;data/result_itsm.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Baca dan parse fail HCL sedia ada Kali ini kita tidak akan bermula dengan fail kosong, sebaliknya mengambil fail HCL yang sedia ada.\n1 2 content, _ := ioutil.ReadFile(\u0026#34;data/\u0026#34; + pod + \u0026#34;.tf\u0026#34;) f, _ := hclwrite.ParseConfig(content, \u0026#34;\u0026#34;, hcl.InitialPos) Kita menggunakan function ReadFile untuk membaca keseluruhan fail tersebut. Function tersebut akan memulangkan content dalam bentuk []byte yang akan kita berikan kepada function hclwrite.ParseConfig(). Function inilah yang bertanggungjawab memahami syntax sedia ada fail HCL tersebut dan membolehkan kita mengubah fail itu dengan tepat. Function ini akan memulangkan objeck hclwrite.File, sama seperti function hclwrite.NewEmptyFile() di bahagian 1.\nCari bahagian untuk kita ubah Terdapat pelbagai cara yang boleh kita gunakan untuk mencari bahagian tertentu yang ingin kita ubah. Antaranya ialah dengan menggunakan function FirstMatchingBlock(). Kita perlu menetapkan jenis (type) block yang ingin dicari, kemudian diikuti dengan label-label yang ada pada block tersebut.\n1 block := f.Body().FirstMatchingBlock(\u0026#34;module\u0026#34;, []string{\u0026#34;team_\u0026#34; + pod + \u0026#34;_team\u0026#34;}) Tambah pengubahan yang diinginkan 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 oldMembers := block.Body().GetAttribute(\u0026#34;members\u0026#34;).Expr().BuildTokens(nil) newEntry := hclwrite.NewExpressionAbsTraversal( hcl.Traversal{ hcl.TraverseRoot{Name: \u0026#34;github_membership\u0026#34;}, hcl.TraverseAttr{Name: mlId}, hcl.TraverseAttr{Name: \u0026#34;username\u0026#34;}, }, ).BuildTokens(nil) newMembers := append( oldMembers[:len(oldMembers)-2], \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, ) newMembers = append(newMembers, newEntry...) newMembers = append(newMembers, hclwrite.Tokens{ \u0026amp;hclwrite.Token{Type: hclsyntax.TokenComma, Bytes: []byte{\u0026#39;,\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenNewline, Bytes: []byte{\u0026#39;\\n\u0026#39;}}, \u0026amp;hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte{\u0026#39;]\u0026#39;}}, }...) block.Body().SetAttributeRaw(\u0026#34;members\u0026#34;, newMembers) Dapatkan nilai attribute yang ingin kita ubah melalui function GetAttribute(). Nilai attribute ini merupakan sebuah expression. Untuk mengubahnya kita perlu menukarkannya kepada Token.\nApa itu Token? TODO\nBeza Traversal dan literal Value Traversal digunakan untuk merujuk kepada variable lain dalam fail HCL tersebut. Literal value tidak merujuk kepada mana-mana bahagian lain dalam fail/projek, berdiri dengan sendiri.\nSimpan fail yang diubah 1 ioutil.WriteFile(\u0026#34;data/result_itsm.tf\u0026#34;, hclwrite.Format(f.Bytes()), 0644) Konklusi Manipulasi fail HCL menggunakan library hclwrite lebih kompleks daripada melakukan ubahsuai secara manual tetapi jika ini perkara yang anda perlu lakukan setiap hari, mungkin lebih senang jika anda meluangkan masa beberapa hari untuk membangunkan solusi automation ini supaya perkara yang sama tidak perlu lagi intervensi manual daripada anda.\nSumber Rujukan Terraform Configuration Syntax hclwrite package documentation Write Terraform Files in Go with hclwrite ","date":"Sep 19","permalink":"https://pokgak.xyz/articles/hclwrite-basics/","tags":null,"title":"Generate fail HCL menggunakan library hclwrite"},{"categories":null,"contents":"Dalam blog post saya sebelum ini saya dah menerangkan bagaimana saya membuat graf animasi perkembangan status pemberian imunisasi negeri-negeri di Malaysia. Seterusnya saya juga ada berkongsi asas-asas untuk menggunakan Github Actions. Dalam blog post ini saya ingin menerangkan pula bagaimana saya menggunakan Github Actions untuk mengemaskini graf tersebut dengan data terbaru yang dikeluarkan oleh pihak CITF Malaysia secara automatik setiap hari.\nKonfigurasi penuh Sebelum saya mula penerangan, inilah hasil fail workflow Github Actions yang saya gunakan:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 name: Update Graphs on: push: branches: [main] workflow_dispatch: schedule: - cron: \u0026#34;0 0 * * *\u0026#34; # Run workflow everyday at 12 AM jobs: vax-count-by-state: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: token: ${{ secrets.GITHUB_TOKEN }} - uses: actions/setup-python@v2 with: python-version: \u0026#34;3.8\u0026#34; - name: Cache pip uses: actions/cache@v2 with: # This path is specific to Ubuntu path: ~/.cache/pip # Look to see if there is a cache hit for the corresponding requirements file key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;requirements.txt\u0026#39;) }} restore-keys: | ${{ runner.os }}-pip- ${{ runner.os }}- - name: Install dependencies run: pip3 install -r requirements.txt - name: Fetch latest data \u0026amp; generate new graph run: python3 main.py - id: get-date run: echo \u0026#34;::set-output name=value::$(date --iso-8601)\u0026#34; - uses: stefanzweifel/git-auto-commit-action@v4 with: commit_message: \u0026#34;bot: update graph for ${{ steps.get-date.outputs.value }}\u0026#34; Bahagian-bahagian Saya akan memecahkan penerangan saya kepada beberapa bahagian iaitu:\nJadual auto-update Melakukan kemaskini graf Commit \u0026amp; Push kemaskini ke repository Bahagian 1: Jadual Auto-update Di bahagian ini saya akan menunjukkan cara bagaimana saya menetapkan Github Actions untuk melakukan kemaskini setiap hari secara automatik.\nDalam penerangan saya berkenaan asas-asas Github Actions, saya ada menyebut yang sesebuah workflow itu boleh dicetuskan oleh pelbagai event daripada Github. Antara event yang disokong adalah menjalankan workflow tersebut berdasarkan jadual yang ditetapkan. Untuk ini kita memerlukan keyword schedule di bawah keyword utama on seperti contoh di bawah:\n1 2 3 on: schedule: - cron: \u0026#34;0 0 * * *\u0026#34; # Run workflow everyday at 12 AM Keyword schedule ini menerima jadual dalam format syntax cron. Jika anda tahu selok-belok sesebuah sistem UNIX atau Linux anda mungkin tahu mengenai cron. Untuk yang belum tahu apa syntax cron itu, ia mempunyai 5 bahagian yang dipisahkan dengan paling kurang satu karakter whitespace seperti space atau tab. Bermula dari kiri, bahagian-bahagian tersebut melambangkan nilai berikut, nilai yang boleh diterima saya letakkan dalam kotak disebelah:\nminit [0 hingga 59] jam [0 hingga 23] hari dalam bulan [1 hingga 31] bulan dalam tahun [1 hingga 12] hari dalam minggu [0 hingga 6], bermula dengan 0=Ahad, 1=Isnin, dan seterusnya hingga 6=Sabtu Nilai khas * boleh digunakan yang membawa maksud untuk setiap nilai dalam bahagian tersebut. Dalam fail workflow saya jadual cron yang digunakan adalah \u0026ldquo;0 0 * * *\u0026rdquo; yang bermakna, \u0026ldquo;Jalankan fail workflow ini pada jam 0:00 (tengah malam) setiap hari dalam bulan, untuk setiap tahun, tidak mengira hari apa pun\u0026rdquo;. Kadangkala syntax cron ini boleh mengelirukan. Jadi saya mencadangkan laman crontab.guru untuk memeriksa dan bereksperimen dengan syntax cron ini.\nBahagian 2: Melakukan kemaskini graf Di blog post sebelum ini saya telah menerangkan code yang saya gunakan untuk menjana graf animasi baru jadi kita akan menggunakan skrip yang sama untuk melakukannya di sini. Walaupun begitu, sebelum menjalankan skrip Python untuk menjana graf berdasarkan informasi baru, kita perlu menyediakan semua perisian yang diperlukan oleh skrip tersebut di Github Actions Runner.\nUntuk itu saya menggunakan [actions/setup-python] untuk menyediakan Python di runner tersebut dan seterusnya menginstall dependency lain. Hanya step terakhir dalam job tersebut adalah bahagian dimana saya betul-betul menjalan kerja tersebut. Berikut adalah code tersebut.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - uses: actions/checkout@v2 with: token: ${{ secrets.GITHUB_TOKEN }} - uses: actions/setup-python@v2 with: python-version: \u0026#34;3.8\u0026#34; - name: Cache pip uses: actions/cache@v2 with: # This path is specific to Ubuntu path: ~/.cache/pip # Look to see if there is a cache hit for the corresponding requirements file key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;requirements.txt\u0026#39;) }} restore-keys: | ${{ runner.os }}-pip- ${{ runner.os }}- - name: Install dependencies run: pip3 install -r requirements.txt - name: Fetch latest data \u0026amp; generate new graph run: python3 main.py Bahagian 3: Commit dan Push kemaskini ke repository Setelah mencipta graf baru dari data terkini daripada repo CITF-public, graf baru kita sudah pun ready tapi belum lagi dipaparkan di website https://pokgak.github.io/citf-graphs kerana ia masih belum dicommit lagi ke repository.\nBiasanya saya akan melakukan commit secara manual dan push ke Github tapi oleh kerana kita melakukan semua proses diatas secara automatik daripada Github Actions, kita tidak boleh lagi buat begitu. Oleh itu, saya menggunakan Actions stefanzweifel/git-auto-commit-action untuk melakukan commit secara automatik. Berikut adalah segmen fail workflow saya yang menunjukkan penggunaan Actions ini:\n1 2 3 - uses: stefanzweifel/git-auto-commit-action@v4 with: commit_message: \u0026#34;bot: update graph for ${{ steps.get-date.outputs.value }}\u0026#34; Seperti yang anda boleh lihat, mudah sahaja cara penggunaan actions ini. Kita hanya perlu menggunakan keyword uses untuk menanda bahawa kita ingin menggunakan Actions luar dalam fail workflow ini, diikuti dengan nama Actions tersebut. Senarai semua actions yang ada boleh dilihat di Github Actions Marketplace. Tambahan pula, anda juga boleh menulis Actions anda sendiri!.\nKonklusi Sejak adanya workflow ini, saya tidak perlu lagi memastikan graf yang saya hasilkan di pokgak/citf-graphs sentiasa dikemaskini dengan maklumat terbaru secara manual, semuanya dilakukan secara automatik. Sejak itu juga, Github menunjukkan aktiviti saya aktif setiap hari, walaupun pada hakikatnya itu semua adalah bot sahaja :p\n","date":"Sep 02","permalink":"https://pokgak.xyz/articles/auto-update-citf-graphs/","tags":["Github Actions","CI/CD"],"title":"Auto-update Graf Covid-19 menggunakan Github Actions"},{"categories":null,"contents":"Perkataan \u0026ldquo;cloud\u0026rdquo; rasanya tak asing lagi dalam kamus anak muda zaman sekarang tapi kebanyakannya merujuk kepada \u0026ldquo;cloud storage\u0026rdquo; iaitu servis penyimpanan data online. Kali ini saya ingin menerangkan konsep cloud computing dari sudut seorang programmer.\nApa itu cloud? \u0026lsquo;The cloud is just someone else\u0026rsquo;s computer\u0026rsquo;, adalah salah satu meme yang banyak ditemui online. Hakikatnya begitulah, gambar yang diupload ke iCloud, website Twitter yang anda akses setiap hari, video yang anda tonton di YouTube, semuanya dilayan (hosted) oleh komputer-komputer yang tersusun di data center besar di seluruh dunia.\nOkay, tapi cloud itu bukan sekadar timbunan komputer seluas berpuluh-puluh padang bola sahaja. Ada beberapa ciri-ciri penting untuk sesuatu itu dipanggil cloud computing.\nCiri-Ciri Cloud Computing Berikut adalah ciri-ciri cloud computing yang utama bagi saya:\nsumber atas permintaan (on-demand resources) kitaran maklum balas pendek (short feedback cycle) berskala infinity (infinitely scalable) ketersediaan global (global availability) Sumber Atas Permintaan (on-demand resources) Ciri ini adalah kelebihan utama apabila menggunakan servis cloud computing. Sebarang sumber yang diperlukan untuk menyebarkan aplikasi anda sama ada server, sistem penyimpanan atau pengkalan data, kebiasaannya boleh disiapkan dalam masa kurang daripada 30 minit. Dengan ini, apabila aplikasi itu telah siap dibangunkan, ia boleh siap tersedia untuk disebarkan dalam masa yang amat singkat.\nDalam artikel Alkisah Syarikat A saya ingin menggambarkan bagaimanakah proses ini dilakukan sebelum ini. Terdapat pelbagai tugas yang perlu dilakukan sebelum sesuatu sumber itu sedia untuk digunakan. Cloud computing mengambil alih tugas ini dari syarikat A.\nKitaran Maklum Balas Pendek (short feedback cycle) Salah satu kelebihan kebolehan menyiapkan sumber atas permintaan adalah kebolehan untuk bertindak dan menyelesaikan masalah kekurangan kapasiti sumber dengan segera. Sebelum ini, masa untuk menyiapkan sumber baru amat besar, oleh sebab itu sumber untuk aplikasi sentiasa disiapkan dengan konfigurasi lebih besar daripada keperluannya hanya supaya mereka mempunyai kapasiti untuk berkembang sebelum perlu dipesan sumber baru.\nTapi dengan kelebihan waktu singkat untuk menyediakan sumber baru, tidak perlu lagi server itu disiapkan dengan konfigurasi lebih dari keperluan. Jika aplikasi itu tiba-tiba mendapat trafik yang tinggi, penyiapan sumber baru untuk menampung kapasiti boleh dilakukan segera.\nBerskala Infiniti (infinitely scalable) Platform-platform hyperscaler cloud computing dikatakan mempunyai sumber infiniti. Pada hakikatnya, apa-apa sumber tidak boleh dikatakan infiniti kerana sumber asli dunia pasti akan habis suatu hari nanti. Tapi infiniti di sini bermaksud, platform-platform hyperscaler ini mampu berkembang lebih cepat daripada kadar keperluan sumber oleh semua pengguna platform tersebut.\nKetersediaan Global (global availability) Secara teori satu sumber yang disebarkan dari sebuah data center di Malaysia punyai kebolehan untuk melayan permintaan daripada seluruh dunia. Hakikatnya, ini akan meninggalkan kesan kepada pengguna-pengguna yang berada di lokasi bertentangan dengan lokasi di mana aplikasi itu dilayan.\nOleh itu, untuk mengembangkan aplikasi ke taraf global, syarikat-syarikat perlu menyediakan server di seluruh dunia supaya permintaan dari pengguna boleh dilayan daripada data center yang terdekat dengan mereka. Bahkan teknologi seperti edge computing wujud hanya untuk mengurangkan masa untuk melayan permintaan pengguna dengan meletakkan server pelayan sedekat mungkin dengan pengguna.\nTapi untuk sesebuah syarikat itu menyediakan server di sebuah lokasi baru tidaklah mudah, terutamanya jika mereka tiada kehadiran fizikal di negara tersebut. Hal-hal regulasi, pembayaran bil dan sebagainya akan menjadi lebih rumit kerana terdapat transaksi rentas negara.\nPlatform hyperscaler cloud computing seperti AWS, Azure, Oracle dll. memudahkan proses ini. Semua aspek fizikal akan diuruskan pihak mereka. Sebagai syarikat pengguna kita hanya perlu membayar sumber-sumber tersebut. Tidak perlu lagi difikirkan aspek-aspek lain.\nKonklusi 4 ciri-ciri di atas pada hemat saya adalah karakteristik utama sesebuah platform cloud computing. Dengan memahami ciri-ciri ini kita dapat menjadikan cloud computing ini sebagai alat untuk menyelesaikan permasalahan yang wujud dalam menyediakan aplikasi kita.\nPemahaman ini amatlah penting kerana tanpa memahami bagaimana teknologi cloud computing ini mampu membantu menyelesaikan masalah yang kita hadapi, kita hanyalah seperti lembu diikat hidung, ikut sahaja apa trend yang orang lain suapkan. Akhirnya masa dan wang dibazirkan tanpa kita memperoleh manfaat apa-apa pun.\n","date":"Aug 25","permalink":"https://pokgak.xyz/articles/cloud-computing/","tags":["Cloud Computing"],"title":"Apa itu Cloud Computing?"},{"categories":null,"contents":"Saya gemar melayari subreddit r/dataisbeautiful dan melihat graf hasil buatan pengguna Reddit lain di sana. Salah satu jenis graf yang saya paling minat adalah apabila graf itu seolah-olah animasi, berubah selaras mengikut jangka masa waktu yang semakin bertambah. Kita boleh melihat perkembangan sesuatu data itu dari mula hingga ke akhir.\nContoh post terbaru di subreddit itu yang mempunyai graf sebegini adalah seperti graf di bawah yang memaparkan Kadar vaksinasi sebahagian daripada negara-negara di seluruh dunia (sayang Malaysia tidak dimasukkan sekali di sini):\nSebelum ini saya menganggap animasi sebegini rumit untuk dilakukan tetapi apabila pihak CITF telah melancarkan public repo di Github bagi data vaksinasi Malaysia, saya memutuskan untuk cuba menghasilkan semula gaya visualisasi ini menggunakan data tersebut.\nSeterusnya saya akan menerangkan langkah-langkah yang diperlukan untuk menghasilkan visualisasi seperti yang di bawah. Sebagai rujukan, code penuh yang saya gunakan di sini boleh didapati di sini.\nPembersihan Data Dalam projek yang melibatkan data sebegini, data boleh datang dari pelbagai sumber dan bentuk. Oleh itu, langkah pertama selalunya adalah pembersihan data. Tujuan langkah ini adalah supaya pada akhirnya kita mempunyai data dalam format yang sesuai dan boleh terus digunakan untuk langkah seterusnya tanpa perlu pemprosesan ekstra apa-apa pun.\nSaya bernasib baik kali ini kerana sumber data yang dibekalkan oleh pihak CITF Malaysia sudah pun berada dalam format CSV yang senang untuk dibaca menggunakan pandas, sebuah library untuk memanipulasi data menggunakan Python. Pihak CITF tidak menawarkan public REST API yang boleh digunakan untuk mengambil (fetch) data tersebut maka saya terpaksa mengambil data menerusi Github. Proses ini kurang sesuai jika anda mahu menapis dahulu data yang diambil tapi untuk kegunaan saya ini, kaedah ini adalah mencukupi.\n1 2 3 STATE_DATA_URL = \u0026#34;https://raw.githubusercontent.com/CITF-Malaysia/citf-public/main/vaccination/vax_state.csv\u0026#34; df = pd.read_csv(StringIO(requests.get(data_url).text)) Function read_csv akan mengambil output data yang diambil dari Github dan menukarkannya ke format DataFrame yang digunakan oleh library pandas. Format DataFrame adalah 2D seakan-akan Excel. Ia mempunyai rows dan columns yang mempunyai data dan menawarkan fungsi-fungsi untuk memanipulasi data tersebut (gabung, pisah, transpose, etc) dengan mudah. Berikut adalah code yang saya gunakan untuk menyiapkan data raw tadi untuk visualisasi:\n1 2 3 4 5 6 df.set_index([\u0026#34;date\u0026#34;, \u0026#34;state\u0026#34;]) .loc[:, [\u0026#34;cumul_partial\u0026#34;, \u0026#34;cumul_full\u0026#34;, \u0026#34;cumul\u0026#34;]] .rename(columns={\u0026#34;cumul_partial\u0026#34;: \u0026#34;partially_vaxed\u0026#34;, \u0026#34;cumul_full\u0026#34;: \u0026#34;fully_vaxed\u0026#34;}) .sort_values(by=\u0026#34;cumul\u0026#34;, ascending=False) .sort_index(level=\u0026#34;date\u0026#34;, sort_remaining=False) .reset_index() Secara ringkasnya,\nset_index: saya menetapkan column \u0026ldquo;date\u0026rdquo; dan \u0026ldquo;state\u0026rdquo; index DataFrame tersebut yang akan saya gunakan nanti untuk mengasingkan data vaksinasi mengikut tarikh dan negeri loc: pilih hanya column yang saya mahu rename: memberikan nama baharu kepada column-column tersebut supaya lebih mudah difahami sort_values: susun semua data vaksinasi mengikut jumlah kumulatif (\u0026ldquo;cumul\u0026rdquo;) sort_index: susun semua data vaksinasi mengikut tarikh reset_index: menjadikan column index dari langkah 1 sebelum ini balik seperti column biasa yang boleh digunakan secara normal Untuk mengetahui lebih lanjut fungsi functions yang saya pakai di sini, bolehlah rujuk kepada pandas API Reference.\nVisualisasi Data menggunakan Plotly Plotly adalah sebuah library yang menawarkan fungsi-fungsi untuk mempermudah pengguna untuk menghasilkan visualisasi interaktif. Ia ditawarkan dalam bahasa Python, R, ataupun JavaScript. Saya berpeluang untuk menggunakan Plotly dalam Python untuk menghasilkan visualisasi untuk thesis bachelor saya dan berdasarkan pengalaman saya, sangat mudah untuk bereksperimen dan menghasilkan graf visualisasi menarik menggunakan library ini.\nCiri Plotly yang sangat bagus adalah Plotly Express. Untuk kebanyakan fungsi visualisasi, Plotly Express sudah cukup pandai menakrif data yang diberikan dan kemudian menghasilkan visualisasi seperti yang dikehendaki. Berikut adalah code yang saya gunakan untuk menghasilkan animasi graf yang saya paparkan di permulaan blog post ini:\n1 2 3 4 5 6 7 8 9 fig = px.bar( state_data, x=\u0026#34;state\u0026#34;, y=[\u0026#34;partially_vaxed\u0026#34;, \u0026#34;fully_vaxed\u0026#34;], animation_frame=\u0026#34;date\u0026#34;, animation_group=\u0026#34;state\u0026#34;, labels={\u0026#34;value\u0026#34;: \u0026#34;Total vaccinated\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;variable\u0026#34;: \u0026#34;Dose Type\u0026#34;}, title=\u0026#34;Vaccination Count in Malaysia by State\u0026#34;, ) Jika anda perasan, saya hanyalah menggunakan satu function sahaja daripada Plotly Express iaitu bar. Function ini digunakan untuk menghasilkan visualisasi graf bar. Sebagai parameter, saya berikan data vaksinasi yang telah dibersihkan dan ditukarkan ke format DataFrame. Menerusi parameter x dan y, saya menetapkan data daripada column manakah dalam DataFrame tersebut yang akan digunakan sebagai paksi X dan paksi Y dalam graf.\nSeterusnya, untuk menghasilkan animasi bergerak, saya menggunakan parameter animation_frame dan ditetapkan column \u0026ldquo;date\u0026rdquo; sebagai nilainya (value). Dengan parameter ini, Plotly akan menghasilkan satu graf untuk setiap nilai dalam column tersebut. Jadi bila saya menggunakan column \u0026ldquo;date\u0026rdquo;, Plotly akan menghasilkan satu graf untuk setiap tarikh dalam data vaksinasi. Untuk menghasilkan animasi, graf-graf ini akan disusun mengikut tarikh dan dipaparkan seolah-olah slideshow. Hasil akhirnya kita akan dapat perkembangan kadar vaksinasi selaras dengan masa.\nParameter animation_frame cukup untuk menghasilkan animasi perkembangan kadar vaksinasi tersebut tetapi animasinya kelihation tidak begitu lancar dan seperti terpotong-potong. Oleh itu, saya juga menggunakan parameter animation_group. Dengan parameter ini, Plotly akan mencuba untuk melancarkan transisi antara dua graf yang dihasilkan berdasakan nilai column dalam animation_frame tadi. Dalam visualisasi graf bar, Plotly akan menunjukkan pertukaran posisi bar tersebut apabila ia berubah kedudukan. Dengan ini animasi kita tadi telah pun menjadi lebih lancar.\nAkhir sekali, parameter labels dan title digunakan untuk menetapkan label yang lebih mesra pembaca untuk legend, paksi, serta tajuk graf.\nKonklusi Saya amat berpuas hati dengan animasi graf ini kerana saya telah belajar cara untuk menghasilkan jenis bentuk graf yang telah saya minati buat sekian lama. Namun begitu, walaupun graf ini kelihatan lebih cantik berbanding graf lain dengan animasi bergerak, saya akui apa yang telah saya hasilkan ini lebih kepada latihan menggunakan library Plotly itu sendiri. Masih banyak aspek yang boleh diperbaiki untuk menyampaikan maklumat menggunakan graf secara tepat dan efektif.\nUntuk mengakses segala code yang telah saya tunjukkan di sini, boleh akses repository pokgak/citf-graphs di Github. Saya juga telah menetapkan jadual berkala supaya graf visualisasi tersebut dikemas kini setiap hari menggunakan Github Actions. Blog post cara saya bagaimana saya buat akan datang.\n","date":"Aug 25","permalink":"https://pokgak.xyz/articles/graf-interaktif-citf-plotly/","tags":null,"title":"Animasi interaktif berdasarkan data CITF menggunakan Plotly"},{"categories":null,"contents":"Github Actions (GA) adalah servis automation yang ditawarkan oleh Github untuk semua penggunanya. Jika anda mempunyai repository public di Github, anda boleh mula menggunakan Github Actions pada saat ini tanpa perlu membayar apa-apa pun!\nBagaimana untuk mula dengan Github Actions? Untuk mula menggunakan Github Actions, anda boleh pergi ke mana-mana repository public yang anda miliki dan seterusnya pergi ke tab Actions.\nJika anda belum pernah setup mana-mana workflow di repository tersebut, anda akan melihat pilihan templates siap yang boleh digunakan untuk pelbagai jenis projek. Sebagai pemula, saya cadangkan anda mula dengan template barebones yang ditawarkan.\nAnda boleh menggunakan editor local di komputer sendiri tapi Github juga ada menawarkan editor online di mana fail workflow anda akan diperiksa formatnya secara langsung sambil anda menaip. Github akan highlight jika fail workflow anda mempunyai kesalahan yang membuatkan workflow anda akan gagal. Selain itu juga, di tepi editor online itu ada dipaparkan documentation ringkas mengenai syntax fail workflow jadi anda tidak perlu lagi tukar-tukar tab untuk semasa menulis fail workflow anda.\nAnatomi fail workflow Github Actions Saya telah beberapa kali menyebut \u0026ldquo;fail workflow\u0026rdquo; dalam perenggan sebelum ini tapi belum pernah menerangkan apakah fail workflow itu. Github Actions menggunakan fail workflow untuk menetapkan bagaimana untuk melakukan automasi. Fail ini ditulis dalam format YAML. Satu ciri-ciri penting yang saya mahu highlight di sini adalah format YAML adalah whitespace-sensitive, bermakna anda perlu pastikan indentation fail workflow anda menggunakan 4 spaces.\nSebelum bermula, ini adalah isi akhir fail workflow contoh kita:\n1 2 3 4 5 6 7 8 9 10 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! - uses: actions/checkout@v2 Ikuti penjelasan saya di bawah untuk memahami apakah yang akan dilakukan apabila workflow ini dijalankan.\nKeyword dalam fail workflow Github Actions Dalam fail workflow anda ada dua top-level keyword yang wajib: on dan jobs.\nKeyword on Satu ciri-ciri penting Github Actions adalah, workflow anda perlu dimulakan melalui \u0026ldquo;triggers\u0026rdquo;. Hampir semua aktiviti yang anda boleh lakukan secara manual di Github boleh dijadikan trigger untuk workflow anda. Sebagai contoh, anda boleh menetapkan workflow untuk dijalankan apabila seseorang telah push codenya ke repo, atau apabila pull request baru dibuka. Ini cara bagaimana anda melakukan kedua-dua contoh tersebut:\n1 2 3 on: push: pull_request: Keyword on digunakan untuk menanda bahawa semua keyword dibawahnya adalah event-event dimana fail workflow anda patut dijalankan. push bermakna apabila seseorang telah push codenya ke repo anda, maka Github Actions akan menjalankan fail workflow tersebut. pull_request pula bermakna jika seseorang telah membuka pull request (PR) baru di repository anda, maka fail workflow tersebut akan dijalankan.\nKedua-dua keyword push dan pull_request ini juga boleh menerima sub-keyword lain untuk tujuan menapis dengan lebih spesifik bila workflow itu patut dijalankan. Antara sub-keyword yang boleh digunakan adalah branches untuk menapis hanya push atau pull request kepada branch yang dinyatakan sahaja. Anda juga boleh menapis mengikut lokasi fail code anda di dalam repo menggunakan sub-keyword paths.\nTerdapat banyak lagi keyword yang anda boleh gunakan untuk trigger workflow anda, jika berminat boleh pergi ke page ini dan ini untuk membaca lebih lanjut.\nKeyword jobs Okay, kita telah tetapkan bila workflow ini patut dijalankan menggunakan keyword on. Seterusnya kita akan menetapkan apa yang workflow ini patut buat menggunakan keyword jobs. Sesebuah workflow mestilah mempunyai paling kurang satu job. Untuk mencipta job baru, anda boleh menggunakan apa-apa perkataan sebagai id cuma perlu dipastikan tiada space. Contohnya seperti berikut:\n1 2 3 jobs: job-pertama: runs-on: ubuntu-latest Di sini, job-pertama adalah id untuk job kita. Seterusnya, setiap job perlulah menetapkan di bawah environment manakah job ini akan dijalankan. Github Actions menawarkan platform Windows, Linux, dan macOS yang anda boleh gunakan secara percuma. Senarai penuh versi yang disokong boleh dibaca di halaman ini. Di sini saya menggunakan ubuntu-latest yang bermakna, job ini akan dijalankan di platform Ubuntu yang terbaru (pada masa tulisan ini adalah Ubuntu 20.04.\nSetelah menetapkan platform, tiba masa untuk kita senaraikan apakah yang patut workflow kita ini buat. Untuk itu kita perlukan keyword steps. Seperti keyword jobs, keyword steps mengandungi sub-keywords yang, satu untuk setiap apa yang kita mahu jalankan.\nSetiap satu step akan dimulakan dengan simbol -. Dalam syntax YAML, ini menandakan bahawa semua keyword di bawah satu - adalah satu bahagian. Keyword run digunakan untuk menjalankan command seolah-olah anda berada di terminal platform yang telah dipilih menggunakan keyword runs-on sebelum ini.\n1 2 3 4 5 6 7 8 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! Dalam contoh di atas, saya telah menetapkan step itu untuk run command echo. Command ini akan print perkataan selepas itu ke terminal anda, dalam kes ini anda akan melihat \u0026ldquo;Hello, world\u0026rdquo; di log result workflow anda nanti. Dalam contoh di atas juga, saya telah menetapkan workflow ini untuk run command echo tapi kali ini dengan perkataan lain pula. Selain daripada keyword run, setiap step juga boleh ditetapkan dengan keyword-keyword lain seperti name, id dan pelbagai lagi. Senarai penuh boleh anda lihat di halaman ini. Fungsi simbol - di sini adalah untuk membantu mengumpul semua keyword yang berkaitan dengan step itu. Setiap simbol - bermakna satu step dalam job itu.\nKeyword uses Kita telah melihat bagaimana cara untuk menjalankan sebarang command melalui keyword run. Untuk sesetengah perkara, sekadar bergantung kepada command mungkin akan membataskan apa yang anda boleh lakukan. Oleh itu, Github Actions juga mempunyai fungsi untuk memanggil code luar dari fail workflow anda. Code ini boleh berasal dari repo yang sama ataupun daripada repo developer lain di Github.\nActions ini boleh ditulis dengan pelbagai cara sama ada menggunakan Javascript atau melalui Docker. Github juga menyediakan marketplace untuk anda mencari actions yang sesuai untuk digunakan dalam fail workflow anda. Github sendiri mempunyai beberapa Actions yang essential seperti checkout untuk checkout git repo anda sewaktu workflow dijalankan dan juga setup-node untuk setup environment node/javascript anda.\nUntuk menggunakan Actions, ada perlu menggunakan keyword uses diikuti dengan nama Actions yang ingin digunakan. Kebanyakan Actions juga mempunya keyword tersendiri yang digunakan untuk memperincikan bagaimana Actions tersebut dijalankan.\n1 2 3 4 5 6 7 8 9 10 jobs: job-pertama: runs-on: ubuntu-latest steps: - run: echo Hello, world! - name: Selamat tinggal dunia run: echo Bye, world! - uses: actions/checkout@v2 Dalam contoh di atas, saya menggunakan Actions dari Github actions/checkout untuk melakukan git checkout repo saya ke sewaktu workflow dijalankan. @v2 di bahagian belakang itu menandakan versi Action tersebut yang ingin saya gunakan. Versi yang ditawarkan oleh Action tersebut boleh disemak di page Releases Action tersebut.\nKonklusi Saya pernah menggunakan Jenkins dan Bitbucket Pipeline dan berdasarkan pengalaman saya Github Actions adalah jauh lebih baik dari kedua-dua produk CI/CD tersebut. Dokumentasi Github Actions yang ditawarkan Github adalah sangat lengkap. Saya paling banyak merujuk halaman Workflow Syntax semasa mula belajar menggunakan Github Actions. Selain itu, halaman-halaman lain dalam Reference ini juga sangat membantu anda ingin mula melakukan perkara yang lebih advance dengan Github Actions.\nAntara contoh automation yang pernah saya lakukan menggunakan Github Actions adalah, menjalankan unit test untuk setiap commit push, memeriksa dan baiki tajuk pull request secara automatik jika tidak memenuhi kriteria anda. Saya juga pernah menggunakan Github Actions workflow untuk melakukan DB dump daripada server dan terus upload ke S3. Pada pandangan saya, Github Actions sangat menarik dan macam-macam yang anda boleh lakukan dengannya.\n","date":"Aug 24","permalink":"https://pokgak.xyz/articles/pengenalan-github-actions/","tags":["Github Actions","CI/CD"],"title":"Pengenalan Github Actions"},{"categories":null,"contents":"","date":"Nov 26","permalink":"https://pokgak.xyz/articles/","tags":null,"title":"Articles"}]